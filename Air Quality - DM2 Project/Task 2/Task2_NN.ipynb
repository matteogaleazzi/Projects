{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Task2_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh6CIHVW-lq0"
      },
      "source": [
        "# **NN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rraQvgtVCN62"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1SiN5DDCN63"
      },
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import defaultdict\n",
        "from scipy.stats.stats import pearsonr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5h4XgpPCN68"
      },
      "source": [
        "df = pd.read_csv('AirQualityUCI.csv', sep=';',decimal=\",\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6sI_YO7CN6_",
        "outputId": "4e6c0a4b-ff04-4d17-e8b1-93740e4d42ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Unnamed: 15</th>\n",
              "      <th>Unnamed: 16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Time  CO(GT)  ...      AH  Unnamed: 15  Unnamed: 16\n",
              "0  10/03/2004  18.00.00     2.6  ...  0.7578          NaN          NaN\n",
              "1  10/03/2004  19.00.00     2.0  ...  0.7255          NaN          NaN\n",
              "2  10/03/2004  20.00.00     2.2  ...  0.7502          NaN          NaN\n",
              "3  10/03/2004  21.00.00     2.2  ...  0.7867          NaN          NaN\n",
              "4  10/03/2004  22.00.00     1.6  ...  0.7888          NaN          NaN\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROc-9nR-CN7P",
        "outputId": "30bc69e6-fb91-41ad-8e91-c7875c93fbb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Unnamed: 15</th>\n",
              "      <th>Unnamed: 16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9466</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9467</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9468</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9469</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9470</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Date Time  CO(GT)  PT08.S1(CO)  ...  RH  AH  Unnamed: 15  Unnamed: 16\n",
              "9466  NaN  NaN     NaN          NaN  ... NaN NaN          NaN          NaN\n",
              "9467  NaN  NaN     NaN          NaN  ... NaN NaN          NaN          NaN\n",
              "9468  NaN  NaN     NaN          NaN  ... NaN NaN          NaN          NaN\n",
              "9469  NaN  NaN     NaN          NaN  ... NaN NaN          NaN          NaN\n",
              "9470  NaN  NaN     NaN          NaN  ... NaN NaN          NaN          NaN\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXHjKsTCN7S",
        "outputId": "28968dc0-d7db-486d-95b1-4dde1dcc0da2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date              object\n",
              "Time              object\n",
              "CO(GT)           float64\n",
              "PT08.S1(CO)      float64\n",
              "NMHC(GT)         float64\n",
              "C6H6(GT)         float64\n",
              "PT08.S2(NMHC)    float64\n",
              "NOx(GT)          float64\n",
              "PT08.S3(NOx)     float64\n",
              "NO2(GT)          float64\n",
              "PT08.S4(NO2)     float64\n",
              "PT08.S5(O3)      float64\n",
              "T                float64\n",
              "RH               float64\n",
              "AH               float64\n",
              "Unnamed: 15      float64\n",
              "Unnamed: 16      float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpXBDR2uCN7V",
        "outputId": "9cfdb79a-90e4-4c44-af36-a0a00c41b774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Unnamed: 15</th>\n",
              "      <th>Unnamed: 16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>9357.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-34.207524</td>\n",
              "      <td>1048.990061</td>\n",
              "      <td>-159.090093</td>\n",
              "      <td>1.865683</td>\n",
              "      <td>894.595276</td>\n",
              "      <td>168.616971</td>\n",
              "      <td>794.990168</td>\n",
              "      <td>58.148873</td>\n",
              "      <td>1391.479641</td>\n",
              "      <td>975.072032</td>\n",
              "      <td>9.778305</td>\n",
              "      <td>39.485380</td>\n",
              "      <td>-6.837604</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>77.657170</td>\n",
              "      <td>329.832710</td>\n",
              "      <td>139.789093</td>\n",
              "      <td>41.380206</td>\n",
              "      <td>342.333252</td>\n",
              "      <td>257.433866</td>\n",
              "      <td>321.993552</td>\n",
              "      <td>126.940455</td>\n",
              "      <td>467.210125</td>\n",
              "      <td>456.938184</td>\n",
              "      <td>43.203623</td>\n",
              "      <td>51.216145</td>\n",
              "      <td>38.976670</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>921.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>711.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>637.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>1185.000000</td>\n",
              "      <td>700.000000</td>\n",
              "      <td>10.900000</td>\n",
              "      <td>34.100000</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.500000</td>\n",
              "      <td>1053.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>7.900000</td>\n",
              "      <td>895.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>794.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>1446.000000</td>\n",
              "      <td>942.000000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>48.600000</td>\n",
              "      <td>0.976800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.600000</td>\n",
              "      <td>1221.000000</td>\n",
              "      <td>-200.000000</td>\n",
              "      <td>13.600000</td>\n",
              "      <td>1105.000000</td>\n",
              "      <td>284.000000</td>\n",
              "      <td>960.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>1662.000000</td>\n",
              "      <td>1255.000000</td>\n",
              "      <td>24.100000</td>\n",
              "      <td>61.900000</td>\n",
              "      <td>1.296200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.900000</td>\n",
              "      <td>2040.000000</td>\n",
              "      <td>1189.000000</td>\n",
              "      <td>63.700000</td>\n",
              "      <td>2214.000000</td>\n",
              "      <td>1479.000000</td>\n",
              "      <td>2683.000000</td>\n",
              "      <td>340.000000</td>\n",
              "      <td>2775.000000</td>\n",
              "      <td>2523.000000</td>\n",
              "      <td>44.600000</td>\n",
              "      <td>88.700000</td>\n",
              "      <td>2.231000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            CO(GT)  PT08.S1(CO)  ...  Unnamed: 15  Unnamed: 16\n",
              "count  9357.000000  9357.000000  ...          0.0          0.0\n",
              "mean    -34.207524  1048.990061  ...          NaN          NaN\n",
              "std      77.657170   329.832710  ...          NaN          NaN\n",
              "min    -200.000000  -200.000000  ...          NaN          NaN\n",
              "25%       0.600000   921.000000  ...          NaN          NaN\n",
              "50%       1.500000  1053.000000  ...          NaN          NaN\n",
              "75%       2.600000  1221.000000  ...          NaN          NaN\n",
              "max      11.900000  2040.000000  ...          NaN          NaN\n",
              "\n",
              "[8 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g_jjRmFCN7X"
      },
      "source": [
        "##### Drop Columns Unnamed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOSAXRZ2CN7X"
      },
      "source": [
        "First of all we saw that there are 2 columns that contain only NaN values, so we dropped out that columns.\n",
        "\n",
        "The columns are labeled as Unnamed: 15 and Unnamed: 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9zxkYFFCN7X"
      },
      "source": [
        "df = df.drop([\"Unnamed: 15\",\"Unnamed: 16\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwX6le7kCN7Z",
        "outputId": "f2b31f08-8c99-4269-e019-163cd18baa86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>NMHC(GT)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Time  CO(GT)  PT08.S1(CO)  ...  PT08.S5(O3)     T    RH      AH\n",
              "0  10/03/2004  18.00.00     2.6       1360.0  ...       1268.0  13.6  48.9  0.7578\n",
              "1  10/03/2004  19.00.00     2.0       1292.0  ...        972.0  13.3  47.7  0.7255\n",
              "2  10/03/2004  20.00.00     2.2       1402.0  ...       1074.0  11.9  54.0  0.7502\n",
              "3  10/03/2004  21.00.00     2.2       1376.0  ...       1203.0  11.0  60.0  0.7867\n",
              "4  10/03/2004  22.00.00     1.6       1272.0  ...       1110.0  11.2  59.6  0.7888\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTFCfZpCN7d"
      },
      "source": [
        "##### Drop column NMHC(GT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArIwl94dCN7d"
      },
      "source": [
        "In this column there are too much -200 values (missing values) so also if we replace with mean of the feature is too much dependent from this missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wx8Z9dtCN7e"
      },
      "source": [
        "df = df.drop([\"NMHC(GT)\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuc81hMQCN7g"
      },
      "source": [
        "##### NaN Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvO4M-SQCN7g"
      },
      "source": [
        "Drop columns with NaN, the last 2, Unnamed:15 and Unnamed:16\n",
        "Drop rows with NaN , there only 1% circa.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbTZDnzXCN7g",
        "outputId": "f02c7e4e-7cf4-4052-bdcb-540bd910abe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             True\n",
              "Time             True\n",
              "CO(GT)           True\n",
              "PT08.S1(CO)      True\n",
              "C6H6(GT)         True\n",
              "PT08.S2(NMHC)    True\n",
              "NOx(GT)          True\n",
              "PT08.S3(NOx)     True\n",
              "NO2(GT)          True\n",
              "PT08.S4(NO2)     True\n",
              "PT08.S5(O3)      True\n",
              "T                True\n",
              "RH               True\n",
              "AH               True\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCuwRH4RCN7i",
        "outputId": "e2d12eac-466d-46dd-a1f0-b5ca18eb7bad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             114\n",
              "Time             114\n",
              "CO(GT)           114\n",
              "PT08.S1(CO)      114\n",
              "C6H6(GT)         114\n",
              "PT08.S2(NMHC)    114\n",
              "NOx(GT)          114\n",
              "PT08.S3(NOx)     114\n",
              "NO2(GT)          114\n",
              "PT08.S4(NO2)     114\n",
              "PT08.S5(O3)      114\n",
              "T                114\n",
              "RH               114\n",
              "AH               114\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oYdkkynCN7k",
        "outputId": "72e2d345-3998-4db9-93a7-b844effa9681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9471, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2dRAnbUCN7m",
        "outputId": "3290e302-f8af-4d49-9dcb-685109dbb416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9466</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9467</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9468</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9469</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9470</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9471 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date      Time  CO(GT)  ...     T    RH      AH\n",
              "0     10/03/2004  18.00.00     2.6  ...  13.6  48.9  0.7578\n",
              "1     10/03/2004  19.00.00     2.0  ...  13.3  47.7  0.7255\n",
              "2     10/03/2004  20.00.00     2.2  ...  11.9  54.0  0.7502\n",
              "3     10/03/2004  21.00.00     2.2  ...  11.0  60.0  0.7867\n",
              "4     10/03/2004  22.00.00     1.6  ...  11.2  59.6  0.7888\n",
              "...          ...       ...     ...  ...   ...   ...     ...\n",
              "9466         NaN       NaN     NaN  ...   NaN   NaN     NaN\n",
              "9467         NaN       NaN     NaN  ...   NaN   NaN     NaN\n",
              "9468         NaN       NaN     NaN  ...   NaN   NaN     NaN\n",
              "9469         NaN       NaN     NaN  ...   NaN   NaN     NaN\n",
              "9470         NaN       NaN     NaN  ...   NaN   NaN     NaN\n",
              "\n",
              "[9471 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egKyN8jqCN7p"
      },
      "source": [
        "df= df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGIqE1G1CN7u",
        "outputId": "3a93adab-c2d8-41d3-ba52-ab3fb80b425b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9357, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnb2gn1hCN7w",
        "outputId": "7630a01f-ae79-46a3-b5ef-1cfaba0ad1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9352</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>10.00.00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1314.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1374.0</td>\n",
              "      <td>1729.0</td>\n",
              "      <td>21.9</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0.7568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9353</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>11.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>11.4</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.7119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9354</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>12.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>18.3</td>\n",
              "      <td>0.6406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9355</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>13.00.00</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>961.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>770.0</td>\n",
              "      <td>28.3</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.5139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9356</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>14.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1047.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>28.5</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.5028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9357 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date      Time  CO(GT)  ...     T    RH      AH\n",
              "0     10/03/2004  18.00.00     2.6  ...  13.6  48.9  0.7578\n",
              "1     10/03/2004  19.00.00     2.0  ...  13.3  47.7  0.7255\n",
              "2     10/03/2004  20.00.00     2.2  ...  11.9  54.0  0.7502\n",
              "3     10/03/2004  21.00.00     2.2  ...  11.0  60.0  0.7867\n",
              "4     10/03/2004  22.00.00     1.6  ...  11.2  59.6  0.7888\n",
              "...          ...       ...     ...  ...   ...   ...     ...\n",
              "9352  04/04/2005  10.00.00     3.1  ...  21.9  29.3  0.7568\n",
              "9353  04/04/2005  11.00.00     2.4  ...  24.3  23.7  0.7119\n",
              "9354  04/04/2005  12.00.00     2.4  ...  26.9  18.3  0.6406\n",
              "9355  04/04/2005  13.00.00     2.1  ...  28.3  13.5  0.5139\n",
              "9356  04/04/2005  14.00.00     2.2  ...  28.5  13.1  0.5028\n",
              "\n",
              "[9357 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuE-xg8SCN7y"
      },
      "source": [
        "After the drop of NaN values in rows we have passed from 9471 rows to 9357 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB_JJ1_2CN7z",
        "outputId": "92e35c9f-b2de-485e-8255-7d6d1ae1d4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date              object\n",
              "Time              object\n",
              "CO(GT)           float64\n",
              "PT08.S1(CO)      float64\n",
              "C6H6(GT)         float64\n",
              "PT08.S2(NMHC)    float64\n",
              "NOx(GT)          float64\n",
              "PT08.S3(NOx)     float64\n",
              "NO2(GT)          float64\n",
              "PT08.S4(NO2)     float64\n",
              "PT08.S5(O3)      float64\n",
              "T                float64\n",
              "RH               float64\n",
              "AH               float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP6y-3gfCN71"
      },
      "source": [
        "##### Missing values with \"-200\" value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC38Ty3GCN71",
        "outputId": "9dff0170-c673-4d0c-98f9-bbe23777131d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.isin([-200.0]).any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             False\n",
              "Time             False\n",
              "CO(GT)            True\n",
              "PT08.S1(CO)       True\n",
              "C6H6(GT)          True\n",
              "PT08.S2(NMHC)     True\n",
              "NOx(GT)           True\n",
              "PT08.S3(NOx)      True\n",
              "NO2(GT)           True\n",
              "PT08.S4(NO2)      True\n",
              "PT08.S5(O3)       True\n",
              "T                 True\n",
              "RH                True\n",
              "AH                True\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny8rXU4bCN73"
      },
      "source": [
        "We know if there is a value with -200 is a missing value by the discription of the dataset, so, we want replace this values with the mean of the each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFM8_FP8CN73"
      },
      "source": [
        "df1=df.replace(-200.0, np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4KMZNUDCN75",
        "outputId": "e1d210f8-9616-45d5-d4e1-9a4e99c42a17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1.isin([-200]).any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             False\n",
              "Time             False\n",
              "CO(GT)           False\n",
              "PT08.S1(CO)      False\n",
              "C6H6(GT)         False\n",
              "PT08.S2(NMHC)    False\n",
              "NOx(GT)          False\n",
              "PT08.S3(NOx)     False\n",
              "NO2(GT)          False\n",
              "PT08.S4(NO2)     False\n",
              "PT08.S5(O3)      False\n",
              "T                False\n",
              "RH               False\n",
              "AH               False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjXTxPAOCN78",
        "outputId": "7ee26138-60e6-4a1c-ac15-2666bac87fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1.isnull().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             False\n",
              "Time             False\n",
              "CO(GT)            True\n",
              "PT08.S1(CO)       True\n",
              "C6H6(GT)          True\n",
              "PT08.S2(NMHC)     True\n",
              "NOx(GT)           True\n",
              "PT08.S3(NOx)      True\n",
              "NO2(GT)           True\n",
              "PT08.S4(NO2)      True\n",
              "PT08.S5(O3)       True\n",
              "T                 True\n",
              "RH                True\n",
              "AH                True\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zj_z5fECN7-"
      },
      "source": [
        "dataframe= df1.fillna(df1.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Gu2TTSCN8A",
        "outputId": "a60e7a25-f88f-4550-ce6c-b709b61ab09a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe.isnull().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             False\n",
              "Time             False\n",
              "CO(GT)           False\n",
              "PT08.S1(CO)      False\n",
              "C6H6(GT)         False\n",
              "PT08.S2(NMHC)    False\n",
              "NOx(GT)          False\n",
              "PT08.S3(NOx)     False\n",
              "NO2(GT)          False\n",
              "PT08.S4(NO2)     False\n",
              "PT08.S5(O3)      False\n",
              "T                False\n",
              "RH               False\n",
              "AH               False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph9mjr1XCN8C",
        "outputId": "f90120ab-a226-4fd3-f008-20b20c9eea89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/03/2004</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9352</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>10.00.00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1314.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1374.0</td>\n",
              "      <td>1729.0</td>\n",
              "      <td>21.9</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0.7568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9353</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>11.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>11.4</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.7119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9354</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>12.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>18.3</td>\n",
              "      <td>0.6406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9355</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>13.00.00</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>961.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>770.0</td>\n",
              "      <td>28.3</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.5139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9356</th>\n",
              "      <td>04/04/2005</td>\n",
              "      <td>14.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1047.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>28.5</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.5028</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9357 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date      Time  CO(GT)  ...     T    RH      AH\n",
              "0     10/03/2004  18.00.00     2.6  ...  13.6  48.9  0.7578\n",
              "1     10/03/2004  19.00.00     2.0  ...  13.3  47.7  0.7255\n",
              "2     10/03/2004  20.00.00     2.2  ...  11.9  54.0  0.7502\n",
              "3     10/03/2004  21.00.00     2.2  ...  11.0  60.0  0.7867\n",
              "4     10/03/2004  22.00.00     1.6  ...  11.2  59.6  0.7888\n",
              "...          ...       ...     ...  ...   ...   ...     ...\n",
              "9352  04/04/2005  10.00.00     3.1  ...  21.9  29.3  0.7568\n",
              "9353  04/04/2005  11.00.00     2.4  ...  24.3  23.7  0.7119\n",
              "9354  04/04/2005  12.00.00     2.4  ...  26.9  18.3  0.6406\n",
              "9355  04/04/2005  13.00.00     2.1  ...  28.3  13.5  0.5139\n",
              "9356  04/04/2005  14.00.00     2.2  ...  28.5  13.1  0.5028\n",
              "\n",
              "[9357 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_alZp4vYCN8E"
      },
      "source": [
        "Now our dataset is called dataframe, we dropped out NaN columns and NMHC(GT) column, we dropped NaN values and we replaced the -200 missing values with the mean of the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiIm9y0ZCN8E"
      },
      "source": [
        "##### Deal with Data and Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPsfBSnPCN8F"
      },
      "source": [
        "In this section we are going to deal with Date and Time format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShmgAr6QCN8F"
      },
      "source": [
        "###### Date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iJfzmtqCN8F"
      },
      "source": [
        "The problem is that i need the format year - month - day to use datetime function, so i have to switch month with day "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU9PUXnBCN8G"
      },
      "source": [
        "dataframe['Date']=pd.to_datetime(dataframe.Date, dayfirst=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSlVtavCN8I"
      },
      "source": [
        "dataframe['Weekday']=dataframe.Date.dt.weekday"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lMwJdckCN8J"
      },
      "source": [
        "dataframe['Month']=dataframe.Date.dt.month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM_rzfdSCN8M",
        "outputId": "bb43eb61-21ae-49ef-dc48-545f4efea576",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe.dtypes\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             datetime64[ns]\n",
              "Time                     object\n",
              "CO(GT)                  float64\n",
              "PT08.S1(CO)             float64\n",
              "C6H6(GT)                float64\n",
              "PT08.S2(NMHC)           float64\n",
              "NOx(GT)                 float64\n",
              "PT08.S3(NOx)            float64\n",
              "NO2(GT)                 float64\n",
              "PT08.S4(NO2)            float64\n",
              "PT08.S5(O3)             float64\n",
              "T                       float64\n",
              "RH                      float64\n",
              "AH                      float64\n",
              "Weekday                   int64\n",
              "Month                     int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9SngFTCN8O",
        "outputId": "8c0c830e-7aa7-42ec-9b64-cd90d7c922d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Weekday</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9352</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>10.00.00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1314.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1374.0</td>\n",
              "      <td>1729.0</td>\n",
              "      <td>21.9</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0.7568</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9353</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>11.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>11.4</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9354</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>12.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>18.3</td>\n",
              "      <td>0.6406</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9355</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>13.00.00</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>961.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>770.0</td>\n",
              "      <td>28.3</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.5139</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9356</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>14.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1047.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>28.5</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.5028</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9357 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date      Time  CO(GT)  PT08.S1(CO)  ...    RH      AH  Weekday  Month\n",
              "0    2004-03-10  18.00.00     2.6       1360.0  ...  48.9  0.7578        2      3\n",
              "1    2004-03-10  19.00.00     2.0       1292.0  ...  47.7  0.7255        2      3\n",
              "2    2004-03-10  20.00.00     2.2       1402.0  ...  54.0  0.7502        2      3\n",
              "3    2004-03-10  21.00.00     2.2       1376.0  ...  60.0  0.7867        2      3\n",
              "4    2004-03-10  22.00.00     1.6       1272.0  ...  59.6  0.7888        2      3\n",
              "...         ...       ...     ...          ...  ...   ...     ...      ...    ...\n",
              "9352 2005-04-04  10.00.00     3.1       1314.0  ...  29.3  0.7568        0      4\n",
              "9353 2005-04-04  11.00.00     2.4       1163.0  ...  23.7  0.7119        0      4\n",
              "9354 2005-04-04  12.00.00     2.4       1142.0  ...  18.3  0.6406        0      4\n",
              "9355 2005-04-04  13.00.00     2.1       1003.0  ...  13.5  0.5139        0      4\n",
              "9356 2005-04-04  14.00.00     2.2       1071.0  ...  13.1  0.5028        0      4\n",
              "\n",
              "[9357 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exMGIFKrCN8P"
      },
      "source": [
        "Now the Data Format it's ok and also i create a new feature Weekday with 0=monday and ..... and 6=Sunday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omoRldYWCN8Q"
      },
      "source": [
        "###### Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Rb_k0uCN8Q"
      },
      "source": [
        "From Time feature i figure out only the hour and assign it to new feature Hour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Wh2Z6lCN8R",
        "outputId": "efacb0ca-ebc7-4ad9-8cff-90cd31d8e5f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe['Hour'] = pd.to_datetime(dataframe['Time'],format= '%H.%M.%S').dt.hour\n",
        "type(dataframe['Time'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIhycLtVCN8S",
        "outputId": "f1e305fe-91f7-449f-8137-63a1f5ab5c58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             datetime64[ns]\n",
              "Time                     object\n",
              "CO(GT)                  float64\n",
              "PT08.S1(CO)             float64\n",
              "C6H6(GT)                float64\n",
              "PT08.S2(NMHC)           float64\n",
              "NOx(GT)                 float64\n",
              "PT08.S3(NOx)            float64\n",
              "NO2(GT)                 float64\n",
              "PT08.S4(NO2)            float64\n",
              "PT08.S5(O3)             float64\n",
              "T                       float64\n",
              "RH                      float64\n",
              "AH                      float64\n",
              "Weekday                   int64\n",
              "Month                     int64\n",
              "Hour                      int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYg5IG9vCN8U",
        "outputId": "8d8e5b8b-99e5-49ba-b47a-f821b0586e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Weekday</th>\n",
              "      <th>Month</th>\n",
              "      <th>Hour</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9352</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>10.00.00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1314.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1374.0</td>\n",
              "      <td>1729.0</td>\n",
              "      <td>21.9</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0.7568</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9353</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>11.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>11.4</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9354</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>12.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>18.3</td>\n",
              "      <td>0.6406</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9355</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>13.00.00</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>961.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>770.0</td>\n",
              "      <td>28.3</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.5139</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9356</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>14.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1047.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>28.5</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.5028</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9357 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date      Time  CO(GT)  PT08.S1(CO)  ...      AH  Weekday  Month  Hour\n",
              "0    2004-03-10  18.00.00     2.6       1360.0  ...  0.7578        2      3    18\n",
              "1    2004-03-10  19.00.00     2.0       1292.0  ...  0.7255        2      3    19\n",
              "2    2004-03-10  20.00.00     2.2       1402.0  ...  0.7502        2      3    20\n",
              "3    2004-03-10  21.00.00     2.2       1376.0  ...  0.7867        2      3    21\n",
              "4    2004-03-10  22.00.00     1.6       1272.0  ...  0.7888        2      3    22\n",
              "...         ...       ...     ...          ...  ...     ...      ...    ...   ...\n",
              "9352 2005-04-04  10.00.00     3.1       1314.0  ...  0.7568        0      4    10\n",
              "9353 2005-04-04  11.00.00     2.4       1163.0  ...  0.7119        0      4    11\n",
              "9354 2005-04-04  12.00.00     2.4       1142.0  ...  0.6406        0      4    12\n",
              "9355 2005-04-04  13.00.00     2.1       1003.0  ...  0.5139        0      4    13\n",
              "9356 2005-04-04  14.00.00     2.2       1071.0  ...  0.5028        0      4    14\n",
              "\n",
              "[9357 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkIYaJcsCN8W"
      },
      "source": [
        "##### New features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OpyG6zsCN8X"
      },
      "source": [
        "At this point we want implement some new features, like 'is night' , 'is evening' , 'officheHour' and 'is weekend' .\n",
        "Aftet that for each variable we transform from boolean to int 0 and 1 .\n",
        "1 == True\n",
        "0 == False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89d6EH1sCN8Y"
      },
      "source": [
        "# dataframe['is Evening'] = (dataframe['Hour'] >= 18) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onn_X-naCN8Z"
      },
      "source": [
        "# dataframe['is Evening'] = dataframe['is Evening'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR6syH6mCN8d"
      },
      "source": [
        "# dataframe['is Night'] = ( (dataframe['Hour'] >= 18) | (dataframe['Hour'] <6) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlN5l_OCN8i"
      },
      "source": [
        "# dataframe['is Night'] = dataframe['is Night'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EPc0DYCCN8l"
      },
      "source": [
        "# dataframe['OfficeHour'] = ( (dataframe['Hour'] >= 9) & (dataframe['Hour'] <=18 ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0rNcA6yCN8n"
      },
      "source": [
        "# dataframe['OfficeHour'] = dataframe['OfficeHour'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQZKkLDTCN8q"
      },
      "source": [
        "dataframe['Weekend'] = ( (dataframe['Weekday'] == 5) | (dataframe['Weekday'] == 6) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCxZ2bPCN8u"
      },
      "source": [
        "dataframe['Weekend'] = dataframe['Weekend'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-80UZ-P9CN8x",
        "outputId": "e75d60fb-c9b4-4762-a9e4-81f6e2184ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>AH</th>\n",
              "      <th>Weekday</th>\n",
              "      <th>Month</th>\n",
              "      <th>Hour</th>\n",
              "      <th>Weekend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>18.00.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0.7578</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>19.00.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0.7255</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>20.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.7502</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>21.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.7867</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-03-10</td>\n",
              "      <td>22.00.00</td>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0.7888</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9352</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>10.00.00</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1314.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>1101.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>539.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>1374.0</td>\n",
              "      <td>1729.0</td>\n",
              "      <td>21.9</td>\n",
              "      <td>29.3</td>\n",
              "      <td>0.7568</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9353</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>11.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1163.0</td>\n",
              "      <td>11.4</td>\n",
              "      <td>1027.0</td>\n",
              "      <td>353.0</td>\n",
              "      <td>604.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>1264.0</td>\n",
              "      <td>1269.0</td>\n",
              "      <td>24.3</td>\n",
              "      <td>23.7</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9354</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>12.00.00</td>\n",
              "      <td>2.4</td>\n",
              "      <td>1142.0</td>\n",
              "      <td>12.4</td>\n",
              "      <td>1063.0</td>\n",
              "      <td>293.0</td>\n",
              "      <td>603.0</td>\n",
              "      <td>175.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>26.9</td>\n",
              "      <td>18.3</td>\n",
              "      <td>0.6406</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9355</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>13.00.00</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1003.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>961.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>702.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>1041.0</td>\n",
              "      <td>770.0</td>\n",
              "      <td>28.3</td>\n",
              "      <td>13.5</td>\n",
              "      <td>0.5139</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9356</th>\n",
              "      <td>2005-04-04</td>\n",
              "      <td>14.00.00</td>\n",
              "      <td>2.2</td>\n",
              "      <td>1071.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1047.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>654.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>816.0</td>\n",
              "      <td>28.5</td>\n",
              "      <td>13.1</td>\n",
              "      <td>0.5028</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9357 rows × 18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date      Time  CO(GT)  PT08.S1(CO)  ...  Weekday  Month  Hour  Weekend\n",
              "0    2004-03-10  18.00.00     2.6       1360.0  ...        2      3    18        0\n",
              "1    2004-03-10  19.00.00     2.0       1292.0  ...        2      3    19        0\n",
              "2    2004-03-10  20.00.00     2.2       1402.0  ...        2      3    20        0\n",
              "3    2004-03-10  21.00.00     2.2       1376.0  ...        2      3    21        0\n",
              "4    2004-03-10  22.00.00     1.6       1272.0  ...        2      3    22        0\n",
              "...         ...       ...     ...          ...  ...      ...    ...   ...      ...\n",
              "9352 2005-04-04  10.00.00     3.1       1314.0  ...        0      4    10        0\n",
              "9353 2005-04-04  11.00.00     2.4       1163.0  ...        0      4    11        0\n",
              "9354 2005-04-04  12.00.00     2.4       1142.0  ...        0      4    12        0\n",
              "9355 2005-04-04  13.00.00     2.1       1003.0  ...        0      4    13        0\n",
              "9356 2005-04-04  14.00.00     2.2       1071.0  ...        0      4    14        0\n",
              "\n",
              "[9357 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6UlHE3UCN80",
        "outputId": "79e0df4d-cf11-471a-affb-e4e902a33071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date             datetime64[ns]\n",
              "Time                     object\n",
              "CO(GT)                  float64\n",
              "PT08.S1(CO)             float64\n",
              "C6H6(GT)                float64\n",
              "PT08.S2(NMHC)           float64\n",
              "NOx(GT)                 float64\n",
              "PT08.S3(NOx)            float64\n",
              "NO2(GT)                 float64\n",
              "PT08.S4(NO2)            float64\n",
              "PT08.S5(O3)             float64\n",
              "T                       float64\n",
              "RH                      float64\n",
              "AH                      float64\n",
              "Weekday                   int64\n",
              "Month                     int64\n",
              "Hour                      int64\n",
              "Weekend                   int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF2E-iQKCN81",
        "outputId": "51482b72-a3d9-4d46-b07a-02734f702601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataframe['Weekend'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6669\n",
              "1    2688\n",
              "Name: Weekend, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2HZeJ32CN84"
      },
      "source": [
        "Lavoreremo su un dataset che contiene solo i gas, la T, Rh e la varibiale dipendente Weekend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLLPjTPtCN85"
      },
      "source": [
        "df2 = dataframe[[\"CO(GT)\", \"PT08.S1(CO)\", \"C6H6(GT)\",\t\"PT08.S2(NMHC)\", \"NOx(GT)\",\t\"PT08.S3(NOx)\",\t\"NO2(GT)\",\t\"PT08.S4(NO2)\",\t\"PT08.S5(O3)\",\"T\"\t,\"RH\"\t,\"Weekend\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xqic_BOCN88",
        "outputId": "9fcaaaee-d8aa-4e5f-f476-e3cc6cb05ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>Weekend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CO(GT)  PT08.S1(CO)  C6H6(GT)  ...     T    RH  Weekend\n",
              "0     2.6       1360.0      11.9  ...  13.6  48.9        0\n",
              "1     2.0       1292.0       9.4  ...  13.3  47.7        0\n",
              "2     2.2       1402.0       9.0  ...  11.9  54.0        0\n",
              "3     2.2       1376.0       9.2  ...  11.0  60.0        0\n",
              "4     1.6       1272.0       6.5  ...  11.2  59.6        0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueWDd9oxBsvV",
        "outputId": "a0abb23b-fc61-450b-e1bb-aa68dfe0d2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "      <th>Weekend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.6</td>\n",
              "      <td>1360.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>1046.0</td>\n",
              "      <td>166.0</td>\n",
              "      <td>1056.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>1268.0</td>\n",
              "      <td>13.6</td>\n",
              "      <td>48.9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>9.4</td>\n",
              "      <td>955.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>1174.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1559.0</td>\n",
              "      <td>972.0</td>\n",
              "      <td>13.3</td>\n",
              "      <td>47.7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1402.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>939.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1140.0</td>\n",
              "      <td>114.0</td>\n",
              "      <td>1555.0</td>\n",
              "      <td>1074.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.2</td>\n",
              "      <td>1376.0</td>\n",
              "      <td>9.2</td>\n",
              "      <td>948.0</td>\n",
              "      <td>172.0</td>\n",
              "      <td>1092.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.6</td>\n",
              "      <td>1272.0</td>\n",
              "      <td>6.5</td>\n",
              "      <td>836.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>1205.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>1490.0</td>\n",
              "      <td>1110.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>59.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CO(GT)  PT08.S1(CO)  C6H6(GT)  ...     T    RH  Weekend\n",
              "0     2.6       1360.0      11.9  ...  13.6  48.9        0\n",
              "1     2.0       1292.0       9.4  ...  13.3  47.7        0\n",
              "2     2.2       1402.0       9.0  ...  11.9  54.0        0\n",
              "3     2.2       1376.0       9.2  ...  11.0  60.0        0\n",
              "4     1.6       1272.0       6.5  ...  11.2  59.6        0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtQn5vlD6Dh2"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3CivT7YJdId"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nUkGgJfKvGw"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZd7RDfVQavK"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XopHQItjMw4"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYcoX-s4EZaR"
      },
      "source": [
        "## **Standardizzazione + split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpd5BXzwvGXm"
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWGSEW9wvGXv"
      },
      "source": [
        "Before standardize i need to drop out the depenedet variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFJUtJrvvGXw",
        "outputId": "593d38e6-f8e9-4fef-d6df-816d38d34242",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "scaler.fit(df2.drop('Weekend',axis=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40n1pfgBvGX3"
      },
      "source": [
        "scaled_features = scaler.transform(df2.drop('Weekend',axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgNoaX5VvGX7",
        "outputId": "f8155a01-8665-42d7-b7a2-637c92fcbacd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df2_feat = pd.DataFrame(scaled_features,columns=df2.columns[:-1])\n",
        "df2_feat.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CO(GT)</th>\n",
              "      <th>PT08.S1(CO)</th>\n",
              "      <th>C6H6(GT)</th>\n",
              "      <th>PT08.S2(NMHC)</th>\n",
              "      <th>NOx(GT)</th>\n",
              "      <th>PT08.S3(NOx)</th>\n",
              "      <th>NO2(GT)</th>\n",
              "      <th>PT08.S4(NO2)</th>\n",
              "      <th>PT08.S5(O3)</th>\n",
              "      <th>T</th>\n",
              "      <th>RH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.339856</td>\n",
              "      <td>1.222702</td>\n",
              "      <td>0.248813</td>\n",
              "      <td>0.408519</td>\n",
              "      <td>-0.418252</td>\n",
              "      <td>0.875962</td>\n",
              "      <td>-0.002078</td>\n",
              "      <td>0.694669</td>\n",
              "      <td>0.627494</td>\n",
              "      <td>-0.544962</td>\n",
              "      <td>-0.019689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.116071</td>\n",
              "      <td>0.903123</td>\n",
              "      <td>-0.093547</td>\n",
              "      <td>0.060588</td>\n",
              "      <td>-0.743974</td>\n",
              "      <td>1.344717</td>\n",
              "      <td>-0.480235</td>\n",
              "      <td>0.302742</td>\n",
              "      <td>-0.130331</td>\n",
              "      <td>-0.579615</td>\n",
              "      <td>-0.090386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.035905</td>\n",
              "      <td>1.420088</td>\n",
              "      <td>-0.148325</td>\n",
              "      <td>-0.000586</td>\n",
              "      <td>-0.599209</td>\n",
              "      <td>1.209652</td>\n",
              "      <td>0.020692</td>\n",
              "      <td>0.290955</td>\n",
              "      <td>0.130812</td>\n",
              "      <td>-0.741330</td>\n",
              "      <td>0.280772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.035905</td>\n",
              "      <td>1.297897</td>\n",
              "      <td>-0.120936</td>\n",
              "      <td>0.033824</td>\n",
              "      <td>-0.387231</td>\n",
              "      <td>1.018972</td>\n",
              "      <td>0.202847</td>\n",
              "      <td>0.376413</td>\n",
              "      <td>0.461080</td>\n",
              "      <td>-0.845290</td>\n",
              "      <td>0.634256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.420023</td>\n",
              "      <td>0.809130</td>\n",
              "      <td>-0.490684</td>\n",
              "      <td>-0.394398</td>\n",
              "      <td>-0.599209</td>\n",
              "      <td>1.467865</td>\n",
              "      <td>0.066230</td>\n",
              "      <td>0.099412</td>\n",
              "      <td>0.222979</td>\n",
              "      <td>-0.822188</td>\n",
              "      <td>0.610691</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     CO(GT)  PT08.S1(CO)  C6H6(GT)  ...  PT08.S5(O3)         T        RH\n",
              "0  0.339856     1.222702  0.248813  ...     0.627494 -0.544962 -0.019689\n",
              "1 -0.116071     0.903123 -0.093547  ...    -0.130331 -0.579615 -0.090386\n",
              "2  0.035905     1.420088 -0.148325  ...     0.130812 -0.741330  0.280772\n",
              "3  0.035905     1.297897 -0.120936  ...     0.461080 -0.845290  0.634256\n",
              "4 -0.420023     0.809130 -0.490684  ...     0.222979 -0.822188  0.610691\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4VP1kp5vGX_"
      },
      "source": [
        "Now i recompute the split on scaled features and without dependent variable:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFbgRQUvGYA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-bq027ivGYE"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(scaled_features,df2['Weekend'],test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrvQSkGVER6K"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVJpbOelGV_m"
      },
      "source": [
        "Il piano è vedere prima il Single Perceptron poi il Multilayer Perceptron esplorando le varie activation funtions: *tanh*, *Logistic* and *identity*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-mtYy5Ecyu"
      },
      "source": [
        "## **Single Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePcu_WlIEzmg"
      },
      "source": [
        "Iniziamo con il single perceptron, i dati sono stati precedentemente normalizzati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lua_WDMOEqFT"
      },
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "ppn = Perceptron(max_iter=150, tol=0.001, eta0=0.01, random_state=10)\n",
        "ppn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ppn.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-J2ddEbEsJs",
        "outputId": "cba4a7ec-690d-4b47-b845-b24f4aaf593d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.719017094017094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyG3RAsRH2-A"
      },
      "source": [
        "Il perceptron con max_iter=150 , tol=0.001 , eta0=0.01 ha dato un accuracy score di 0.72"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL2aUUR4IEiz"
      },
      "source": [
        "## **Sklearn Multilayer Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njnGFdPEIKxw"
      },
      "source": [
        "Iniziamo esplorando la libreria di Scikit-learn con MLPClassifier e vediamo cosa viene fuori con i valori di default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIOtszQVIDts",
        "outputId": "7fa10071-e3ab-473e-a482-4da4308f132e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf = MLPClassifier(random_state=10)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.8205128205128205\n",
            "F1-score [0.88317107 0.61290323]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88      1368\n",
            "           1       0.73      0.53      0.61       504\n",
            "\n",
            "    accuracy                           0.82      1872\n",
            "   macro avg       0.79      0.73      0.75      1872\n",
            "weighted avg       0.81      0.82      0.81      1872\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkRKwKfSIpE-",
        "outputId": "cc87c94b-d69a-4d9c-f1f0-25249ada5a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "plt.plot(clf.loss_curve_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWDjcFeuIqlA"
      },
      "source": [
        "ora voglio andare a provare con hidden layers diversi cercare la migliore activation funcion che la libreria di Scikit-Learn mette a disposizione.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjQLxl29IyN1"
      },
      "source": [
        "Per gli hidden layers non so quali scegliere, da notebook prof sono evidenziati 3 valori:\n",
        "- 128\n",
        "- 64 \n",
        "- 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJGKHDK0JU-L"
      },
      "source": [
        "Non avendo capito a pieno come scelgiere gli hidden layers uso quelli dettati dal prof (128,64,32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XEHoZL7JgyO"
      },
      "source": [
        "### Activation Function : Logistic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJIANnY2JgNF",
        "outputId": "6bba6068-4d71-4b49-e1a9-02e3a707d8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(128, 64, 32,), alpha=0.1, learning_rate='adaptive', \n",
        "                    activation='logistic', early_stopping=False, momentum=0.9, random_state=10)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.7548076923076923\n",
            "F1-score [0.85451664 0.22071307]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.99      0.85      1368\n",
            "           1       0.76      0.13      0.22       504\n",
            "\n",
            "    accuracy                           0.75      1872\n",
            "   macro avg       0.76      0.56      0.54      1872\n",
            "weighted avg       0.76      0.75      0.68      1872\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uULON2tFJ4Nx"
      },
      "source": [
        "Questo è peggio di prima del caso di default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsQtMS6vEiNC",
        "outputId": "1e861e54-05af-471e-edbc-9402118e3342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(clf.loss_curve_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bwl1ax-KB2p"
      },
      "source": [
        "### Activation Function : tanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SJ-vnECKB2q",
        "outputId": "223b8c2b-37f3-4679-bdfc-0ae566cfb1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(128, 64, 32,), alpha=0.1, learning_rate='adaptive', \n",
        "                    activation='tanh', early_stopping=False, momentum=0.9, random_state=10)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.8424145299145299\n",
            "F1-score [0.8919018  0.70935961]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      1368\n",
            "           1       0.70      0.71      0.71       504\n",
            "\n",
            "    accuracy                           0.84      1872\n",
            "   macro avg       0.80      0.80      0.80      1872\n",
            "weighted avg       0.84      0.84      0.84      1872\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n94IeOOkKB2w"
      },
      "source": [
        "Questo è il meglio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQaUfAxjKB2x",
        "outputId": "e58b7719-3c41-4899-badd-fe890a261281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(clf.loss_curve_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar2hFBTAKsxb"
      },
      "source": [
        "### Activation Function : Identity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZGz1ygbKq0J",
        "outputId": "f09e707f-c039-422b-9efe-38a5b1ae4ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(128, 64, 32,), alpha=0.1, learning_rate='adaptive', \n",
        "                    activation='identity', early_stopping=False, momentum=0.9, random_state=10)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
        "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.7430555555555556\n",
            "F1-score [0.83875293 0.36793693]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.91      0.84      1368\n",
            "           1       0.54      0.28      0.37       504\n",
            "\n",
            "    accuracy                           0.74      1872\n",
            "   macro avg       0.66      0.60      0.60      1872\n",
            "weighted avg       0.71      0.74      0.71      1872\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuRKOkAZKzrT",
        "outputId": "e3d94a96-793a-4753-d51b-e33adb115159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(clf.loss_curve_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq5hsDkVLV83"
      },
      "source": [
        "**Il multilayer Percepton migliore è il classificatore con activation: tanh**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDZ2wu0_LejV"
      },
      "source": [
        "## **Keras Deep Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oe6gjG9O_hs"
      },
      "source": [
        "Per quanto riguarda il Deep Neural Network, la rete neurale è stata sviluppata utilizzando la liberia Keras, in particolare i modelli Sequential e Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFKTCOD_Laq3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzh5FZrxMQHd"
      },
      "source": [
        "def build_model():\n",
        "    \n",
        "    n_feature = X_train.shape[1]\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=n_feature, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpRV3fUUPWbL"
      },
      "source": [
        "Come si può vedere dal codice Il primo layer è stato impostato come Sequential e gli altri 3 modelli come Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyc1CUjHMTXQ",
        "outputId": "c867061c-c849-4dea-c398-4a5dd41d6be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model1 = build_model()\n",
        "\n",
        "history1 = model1.fit(X_train, y_train, epochs=50, batch_size=10).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.7325\n",
            "Epoch 2/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.5004 - accuracy: 0.7540\n",
            "Epoch 3/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4868 - accuracy: 0.7598\n",
            "Epoch 4/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4744 - accuracy: 0.7651\n",
            "Epoch 5/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4622 - accuracy: 0.7743\n",
            "Epoch 6/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4523 - accuracy: 0.7789\n",
            "Epoch 7/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4430 - accuracy: 0.7833\n",
            "Epoch 8/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.7894\n",
            "Epoch 9/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4276 - accuracy: 0.7997\n",
            "Epoch 10/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4175 - accuracy: 0.7980\n",
            "Epoch 11/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4116 - accuracy: 0.8063\n",
            "Epoch 12/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8118\n",
            "Epoch 13/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3968 - accuracy: 0.8131\n",
            "Epoch 14/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3899 - accuracy: 0.8187\n",
            "Epoch 15/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3799 - accuracy: 0.8227\n",
            "Epoch 16/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3765 - accuracy: 0.8222\n",
            "Epoch 17/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3697 - accuracy: 0.8293\n",
            "Epoch 18/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3614 - accuracy: 0.8330\n",
            "Epoch 19/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3579 - accuracy: 0.8310\n",
            "Epoch 20/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3555 - accuracy: 0.8350\n",
            "Epoch 21/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3448 - accuracy: 0.8421\n",
            "Epoch 22/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3457 - accuracy: 0.8429\n",
            "Epoch 23/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3407 - accuracy: 0.8420\n",
            "Epoch 24/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3345 - accuracy: 0.8462\n",
            "Epoch 25/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3312 - accuracy: 0.8466\n",
            "Epoch 26/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3263 - accuracy: 0.8529\n",
            "Epoch 27/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3225 - accuracy: 0.8524\n",
            "Epoch 28/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3191 - accuracy: 0.8534\n",
            "Epoch 29/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3133 - accuracy: 0.8572\n",
            "Epoch 30/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3109 - accuracy: 0.8600\n",
            "Epoch 31/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3100 - accuracy: 0.8574\n",
            "Epoch 32/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3062 - accuracy: 0.8617\n",
            "Epoch 33/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2940 - accuracy: 0.8671\n",
            "Epoch 34/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2979 - accuracy: 0.8631\n",
            "Epoch 35/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2955 - accuracy: 0.8679\n",
            "Epoch 36/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2865 - accuracy: 0.8695\n",
            "Epoch 37/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2880 - accuracy: 0.8693\n",
            "Epoch 38/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2849 - accuracy: 0.8744\n",
            "Epoch 39/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2796 - accuracy: 0.8745\n",
            "Epoch 40/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2754 - accuracy: 0.8754\n",
            "Epoch 41/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2712 - accuracy: 0.8783\n",
            "Epoch 42/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2700 - accuracy: 0.8803\n",
            "Epoch 43/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2652 - accuracy: 0.8818\n",
            "Epoch 44/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2659 - accuracy: 0.8807\n",
            "Epoch 45/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2589 - accuracy: 0.8847\n",
            "Epoch 46/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2602 - accuracy: 0.8863\n",
            "Epoch 47/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2518 - accuracy: 0.8915\n",
            "Epoch 48/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2522 - accuracy: 0.8896\n",
            "Epoch 49/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2466 - accuracy: 0.8882\n",
            "Epoch 50/50\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2428 - accuracy: 0.8934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYb3_kZ9MenK",
        "outputId": "87355115-916b-431b-8402-5c13c0bb0baa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model2 = build_model()\n",
        "\n",
        "history2 = model2.fit(X_train, y_train, epochs=50, batch_size=50).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.5470 - accuracy: 0.7173\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.5094 - accuracy: 0.7460\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7585\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.7637\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7702\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7725\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.7773\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7794\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.7838\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7864\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.7904\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4288 - accuracy: 0.7957\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.7989\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4167 - accuracy: 0.8021\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8095\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8095\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8114\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.8150\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8170\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8194\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8210\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8250\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8279\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8253\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8305\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8331\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8329\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8326\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8363\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8379\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8420\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8446\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8456\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8462\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8485\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8452\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8480\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8457\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8509\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3256 - accuracy: 0.8540\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8525\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8545\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8548\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8542\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8544\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8604\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8565\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8595\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8574\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfz51WyPyq5"
      },
      "source": [
        "Come ottimizzatore, è stato scelto il parametro ‘ADAM’, invece come funzione di Loss, ossia la **funzione obiettivo** che il modello tenta di minimizzare, è stata scelta ‘binary_crossentropy’ (ho lasciato quella scelta da prof).\n",
        "\n",
        "Vediamo come andata:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-Ymt4eCMieY",
        "outputId": "61865d25-c399-4b4f-8f75-b5c0760b8110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(history1['loss'], label='Loss 10')\n",
        "plt.plot(history2['loss'], label='Loss 50')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrNQRvMfQKO9",
        "outputId": "2664f867-e7cc-4186-f4e0-37be7033109d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss_1, test_acc_1 = model1.evaluate(X_test, y_test)\n",
        "test_loss_2, test_acc_2 = model2.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss %f, Accuracy %f' % (test_loss_1, test_acc_1))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_2, test_acc_2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 799us/step - loss: 0.3398 - accuracy: 0.8499\n",
            "59/59 [==============================] - 0s 766us/step - loss: 0.3746 - accuracy: 0.8360\n",
            "Loss 0.339812, Accuracy 0.849893\n",
            "Loss 0.374640, Accuracy 0.836004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPLL90XqQWm2"
      },
      "source": [
        "La variazione della dimensione del batch non ha apportato grandi differenze, infatti i modelli sono alquanto simili come dimostrano l’accuracy e il loss.\n",
        "\n",
        "Batch dim=10 :\n",
        "- Accuracy 0.0.849893\n",
        "- Loss 0.339812\n",
        "\n",
        "\n",
        "Batch dim=50 :\n",
        "- Accuracy 0.836004\n",
        "- Loss 0.374640\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdAcNBJTRSXQ"
      },
      "source": [
        "Ora invece proviamo estraendo dal training set dei records da utilizzare come validation set: l’80% delle istanze è stato usato per addestrare il\n",
        "modello e il restante 20% per validarlo.\n",
        "\n",
        "Si va anche su con le epoche fino a 1000 tenendo un batch_size di 10, mi aspetto di avere risultati migliori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmm6YSBvRIQG",
        "outputId": "f5e814ca-028a-4910-9cab-ddfeca91ca4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model3 = build_model()\n",
        "\n",
        "history3 = model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=10).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.5321 - accuracy: 0.7253 - val_loss: 0.4880 - val_accuracy: 0.7623\n",
            "Epoch 2/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4985 - accuracy: 0.7613 - val_loss: 0.4900 - val_accuracy: 0.7591\n",
            "Epoch 3/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4839 - accuracy: 0.7625 - val_loss: 0.4759 - val_accuracy: 0.7682\n",
            "Epoch 4/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4701 - accuracy: 0.7746 - val_loss: 0.4607 - val_accuracy: 0.7794\n",
            "Epoch 5/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4613 - accuracy: 0.7774 - val_loss: 0.4454 - val_accuracy: 0.7863\n",
            "Epoch 6/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4492 - accuracy: 0.7844 - val_loss: 0.4372 - val_accuracy: 0.7970\n",
            "Epoch 7/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4400 - accuracy: 0.7897 - val_loss: 0.4399 - val_accuracy: 0.7788\n",
            "Epoch 8/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4295 - accuracy: 0.7965 - val_loss: 0.4334 - val_accuracy: 0.7922\n",
            "Epoch 9/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4211 - accuracy: 0.8031 - val_loss: 0.4270 - val_accuracy: 0.7975\n",
            "Epoch 10/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4148 - accuracy: 0.8055 - val_loss: 0.4205 - val_accuracy: 0.8066\n",
            "Epoch 11/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4088 - accuracy: 0.8057 - val_loss: 0.4265 - val_accuracy: 0.8029\n",
            "Epoch 12/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.4051 - accuracy: 0.8111 - val_loss: 0.4157 - val_accuracy: 0.8024\n",
            "Epoch 13/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3956 - accuracy: 0.8080 - val_loss: 0.4187 - val_accuracy: 0.8050\n",
            "Epoch 14/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3896 - accuracy: 0.8210 - val_loss: 0.4098 - val_accuracy: 0.8093\n",
            "Epoch 15/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3858 - accuracy: 0.8208 - val_loss: 0.4182 - val_accuracy: 0.8050\n",
            "Epoch 16/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3812 - accuracy: 0.8202 - val_loss: 0.4019 - val_accuracy: 0.8210\n",
            "Epoch 17/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3763 - accuracy: 0.8198 - val_loss: 0.3984 - val_accuracy: 0.8130\n",
            "Epoch 18/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3686 - accuracy: 0.8246 - val_loss: 0.4024 - val_accuracy: 0.8104\n",
            "Epoch 19/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3637 - accuracy: 0.8279 - val_loss: 0.4176 - val_accuracy: 0.8082\n",
            "Epoch 20/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3612 - accuracy: 0.8325 - val_loss: 0.3939 - val_accuracy: 0.8162\n",
            "Epoch 21/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3532 - accuracy: 0.8374 - val_loss: 0.3937 - val_accuracy: 0.8194\n",
            "Epoch 22/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3486 - accuracy: 0.8386 - val_loss: 0.3904 - val_accuracy: 0.8269\n",
            "Epoch 23/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3458 - accuracy: 0.8428 - val_loss: 0.4002 - val_accuracy: 0.8221\n",
            "Epoch 24/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3414 - accuracy: 0.8441 - val_loss: 0.3803 - val_accuracy: 0.8269\n",
            "Epoch 25/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3383 - accuracy: 0.8460 - val_loss: 0.3857 - val_accuracy: 0.8221\n",
            "Epoch 26/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3306 - accuracy: 0.8478 - val_loss: 0.4005 - val_accuracy: 0.8194\n",
            "Epoch 27/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3322 - accuracy: 0.8450 - val_loss: 0.3914 - val_accuracy: 0.8275\n",
            "Epoch 28/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3233 - accuracy: 0.8541 - val_loss: 0.3770 - val_accuracy: 0.8291\n",
            "Epoch 29/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3188 - accuracy: 0.8508 - val_loss: 0.3854 - val_accuracy: 0.8146\n",
            "Epoch 30/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3220 - accuracy: 0.8548 - val_loss: 0.3935 - val_accuracy: 0.8189\n",
            "Epoch 31/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3148 - accuracy: 0.8591 - val_loss: 0.3881 - val_accuracy: 0.8296\n",
            "Epoch 32/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3099 - accuracy: 0.8592 - val_loss: 0.3733 - val_accuracy: 0.8312\n",
            "Epoch 33/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3066 - accuracy: 0.8605 - val_loss: 0.3856 - val_accuracy: 0.8232\n",
            "Epoch 34/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.3037 - accuracy: 0.8625 - val_loss: 0.3660 - val_accuracy: 0.8307\n",
            "Epoch 35/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2997 - accuracy: 0.8629 - val_loss: 0.3698 - val_accuracy: 0.8307\n",
            "Epoch 36/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2938 - accuracy: 0.8687 - val_loss: 0.3957 - val_accuracy: 0.8157\n",
            "Epoch 37/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2909 - accuracy: 0.8705 - val_loss: 0.3893 - val_accuracy: 0.8168\n",
            "Epoch 38/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2942 - accuracy: 0.8653 - val_loss: 0.3679 - val_accuracy: 0.8307\n",
            "Epoch 39/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2880 - accuracy: 0.8725 - val_loss: 0.3694 - val_accuracy: 0.8392\n",
            "Epoch 40/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2845 - accuracy: 0.8721 - val_loss: 0.3781 - val_accuracy: 0.8259\n",
            "Epoch 41/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2802 - accuracy: 0.8728 - val_loss: 0.3620 - val_accuracy: 0.8349\n",
            "Epoch 42/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2772 - accuracy: 0.8756 - val_loss: 0.3865 - val_accuracy: 0.8248\n",
            "Epoch 43/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2737 - accuracy: 0.8767 - val_loss: 0.3611 - val_accuracy: 0.8424\n",
            "Epoch 44/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2706 - accuracy: 0.8818 - val_loss: 0.3706 - val_accuracy: 0.8291\n",
            "Epoch 45/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2679 - accuracy: 0.8812 - val_loss: 0.3819 - val_accuracy: 0.8360\n",
            "Epoch 46/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2627 - accuracy: 0.8858 - val_loss: 0.3734 - val_accuracy: 0.8392\n",
            "Epoch 47/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2596 - accuracy: 0.8852 - val_loss: 0.3914 - val_accuracy: 0.8226\n",
            "Epoch 48/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2623 - accuracy: 0.8854 - val_loss: 0.3607 - val_accuracy: 0.8419\n",
            "Epoch 49/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2563 - accuracy: 0.8866 - val_loss: 0.3733 - val_accuracy: 0.8408\n",
            "Epoch 50/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2501 - accuracy: 0.8872 - val_loss: 0.3843 - val_accuracy: 0.8328\n",
            "Epoch 51/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2492 - accuracy: 0.8927 - val_loss: 0.3564 - val_accuracy: 0.8446\n",
            "Epoch 52/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2470 - accuracy: 0.8930 - val_loss: 0.3703 - val_accuracy: 0.8451\n",
            "Epoch 53/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2440 - accuracy: 0.8931 - val_loss: 0.3666 - val_accuracy: 0.8467\n",
            "Epoch 54/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2419 - accuracy: 0.8955 - val_loss: 0.3566 - val_accuracy: 0.8494\n",
            "Epoch 55/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2390 - accuracy: 0.8955 - val_loss: 0.3592 - val_accuracy: 0.8435\n",
            "Epoch 56/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2356 - accuracy: 0.8977 - val_loss: 0.3740 - val_accuracy: 0.8472\n",
            "Epoch 57/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2322 - accuracy: 0.9010 - val_loss: 0.3748 - val_accuracy: 0.8419\n",
            "Epoch 58/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2323 - accuracy: 0.8997 - val_loss: 0.3794 - val_accuracy: 0.8446\n",
            "Epoch 59/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2313 - accuracy: 0.9014 - val_loss: 0.3690 - val_accuracy: 0.8387\n",
            "Epoch 60/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2313 - accuracy: 0.8975 - val_loss: 0.3666 - val_accuracy: 0.8435\n",
            "Epoch 61/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.2198 - accuracy: 0.9050 - val_loss: 0.3799 - val_accuracy: 0.8440\n",
            "Epoch 62/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2225 - accuracy: 0.9001 - val_loss: 0.3877 - val_accuracy: 0.8408\n",
            "Epoch 63/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2224 - accuracy: 0.9057 - val_loss: 0.3764 - val_accuracy: 0.8413\n",
            "Epoch 64/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2212 - accuracy: 0.9059 - val_loss: 0.3781 - val_accuracy: 0.8397\n",
            "Epoch 65/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2159 - accuracy: 0.9084 - val_loss: 0.3821 - val_accuracy: 0.8413\n",
            "Epoch 66/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2155 - accuracy: 0.9109 - val_loss: 0.3776 - val_accuracy: 0.8381\n",
            "Epoch 67/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2141 - accuracy: 0.9078 - val_loss: 0.3813 - val_accuracy: 0.8429\n",
            "Epoch 68/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2095 - accuracy: 0.9073 - val_loss: 0.3851 - val_accuracy: 0.8462\n",
            "Epoch 69/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2058 - accuracy: 0.9133 - val_loss: 0.3826 - val_accuracy: 0.8440\n",
            "Epoch 70/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2083 - accuracy: 0.9092 - val_loss: 0.3961 - val_accuracy: 0.8419\n",
            "Epoch 71/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2045 - accuracy: 0.9133 - val_loss: 0.4053 - val_accuracy: 0.8339\n",
            "Epoch 72/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2053 - accuracy: 0.9114 - val_loss: 0.3699 - val_accuracy: 0.8494\n",
            "Epoch 73/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.2002 - accuracy: 0.9150 - val_loss: 0.4083 - val_accuracy: 0.8328\n",
            "Epoch 74/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1983 - accuracy: 0.9158 - val_loss: 0.3816 - val_accuracy: 0.8424\n",
            "Epoch 75/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1981 - accuracy: 0.9166 - val_loss: 0.3940 - val_accuracy: 0.8381\n",
            "Epoch 76/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1969 - accuracy: 0.9145 - val_loss: 0.4031 - val_accuracy: 0.8328\n",
            "Epoch 77/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1955 - accuracy: 0.9180 - val_loss: 0.3786 - val_accuracy: 0.8531\n",
            "Epoch 78/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1850 - accuracy: 0.9229 - val_loss: 0.3830 - val_accuracy: 0.8531\n",
            "Epoch 79/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1889 - accuracy: 0.9214 - val_loss: 0.3672 - val_accuracy: 0.8510\n",
            "Epoch 80/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1875 - accuracy: 0.9208 - val_loss: 0.4208 - val_accuracy: 0.8339\n",
            "Epoch 81/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1863 - accuracy: 0.9222 - val_loss: 0.4069 - val_accuracy: 0.8387\n",
            "Epoch 82/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1901 - accuracy: 0.9204 - val_loss: 0.3826 - val_accuracy: 0.8488\n",
            "Epoch 83/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1776 - accuracy: 0.9303 - val_loss: 0.3899 - val_accuracy: 0.8424\n",
            "Epoch 84/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.9246 - val_loss: 0.3998 - val_accuracy: 0.8542\n",
            "Epoch 85/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1801 - accuracy: 0.9246 - val_loss: 0.3799 - val_accuracy: 0.8462\n",
            "Epoch 86/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1777 - accuracy: 0.9272 - val_loss: 0.3907 - val_accuracy: 0.8579\n",
            "Epoch 87/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1739 - accuracy: 0.9289 - val_loss: 0.4063 - val_accuracy: 0.8429\n",
            "Epoch 88/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1808 - accuracy: 0.9249 - val_loss: 0.4085 - val_accuracy: 0.8429\n",
            "Epoch 89/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.9273 - val_loss: 0.4131 - val_accuracy: 0.8494\n",
            "Epoch 90/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9236 - val_loss: 0.4188 - val_accuracy: 0.8590\n",
            "Epoch 91/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.1648 - accuracy: 0.9316 - val_loss: 0.3987 - val_accuracy: 0.8552\n",
            "Epoch 92/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.1647 - accuracy: 0.9339 - val_loss: 0.3944 - val_accuracy: 0.8654\n",
            "Epoch 93/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.1724 - accuracy: 0.9287 - val_loss: 0.4139 - val_accuracy: 0.8494\n",
            "Epoch 94/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1639 - accuracy: 0.9339 - val_loss: 0.4121 - val_accuracy: 0.8494\n",
            "Epoch 95/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1600 - accuracy: 0.9359 - val_loss: 0.4469 - val_accuracy: 0.8424\n",
            "Epoch 96/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1616 - accuracy: 0.9333 - val_loss: 0.3879 - val_accuracy: 0.8467\n",
            "Epoch 97/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1618 - accuracy: 0.9347 - val_loss: 0.4302 - val_accuracy: 0.8488\n",
            "Epoch 98/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1606 - accuracy: 0.9345 - val_loss: 0.3894 - val_accuracy: 0.8547\n",
            "Epoch 99/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1555 - accuracy: 0.9372 - val_loss: 0.4021 - val_accuracy: 0.8499\n",
            "Epoch 100/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1546 - accuracy: 0.9380 - val_loss: 0.4305 - val_accuracy: 0.8547\n",
            "Epoch 101/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1550 - accuracy: 0.9372 - val_loss: 0.4026 - val_accuracy: 0.8600\n",
            "Epoch 102/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1584 - accuracy: 0.9348 - val_loss: 0.4191 - val_accuracy: 0.8547\n",
            "Epoch 103/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1536 - accuracy: 0.9384 - val_loss: 0.4190 - val_accuracy: 0.8536\n",
            "Epoch 104/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1470 - accuracy: 0.9396 - val_loss: 0.4192 - val_accuracy: 0.8504\n",
            "Epoch 105/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1555 - accuracy: 0.9368 - val_loss: 0.4256 - val_accuracy: 0.8616\n",
            "Epoch 106/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1451 - accuracy: 0.9401 - val_loss: 0.4330 - val_accuracy: 0.8435\n",
            "Epoch 107/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1493 - accuracy: 0.9404 - val_loss: 0.4365 - val_accuracy: 0.8456\n",
            "Epoch 108/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1488 - accuracy: 0.9416 - val_loss: 0.4430 - val_accuracy: 0.8446\n",
            "Epoch 109/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1426 - accuracy: 0.9418 - val_loss: 0.4263 - val_accuracy: 0.8499\n",
            "Epoch 110/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1481 - accuracy: 0.9408 - val_loss: 0.4419 - val_accuracy: 0.8542\n",
            "Epoch 111/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1403 - accuracy: 0.9452 - val_loss: 0.4788 - val_accuracy: 0.8344\n",
            "Epoch 112/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1415 - accuracy: 0.9426 - val_loss: 0.4351 - val_accuracy: 0.8435\n",
            "Epoch 113/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.9447 - val_loss: 0.4202 - val_accuracy: 0.8536\n",
            "Epoch 114/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1375 - accuracy: 0.9444 - val_loss: 0.4322 - val_accuracy: 0.8520\n",
            "Epoch 115/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1413 - accuracy: 0.9451 - val_loss: 0.4521 - val_accuracy: 0.8419\n",
            "Epoch 116/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1392 - accuracy: 0.9420 - val_loss: 0.4326 - val_accuracy: 0.8547\n",
            "Epoch 117/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1325 - accuracy: 0.9475 - val_loss: 0.4603 - val_accuracy: 0.8536\n",
            "Epoch 118/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1331 - accuracy: 0.9472 - val_loss: 0.4469 - val_accuracy: 0.8499\n",
            "Epoch 119/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1394 - accuracy: 0.9471 - val_loss: 0.4328 - val_accuracy: 0.8531\n",
            "Epoch 120/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1272 - accuracy: 0.9487 - val_loss: 0.4389 - val_accuracy: 0.8606\n",
            "Epoch 121/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1353 - accuracy: 0.9470 - val_loss: 0.4452 - val_accuracy: 0.8616\n",
            "Epoch 122/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.9516 - val_loss: 0.4658 - val_accuracy: 0.8467\n",
            "Epoch 123/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1322 - accuracy: 0.9464 - val_loss: 0.4569 - val_accuracy: 0.8488\n",
            "Epoch 124/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1273 - accuracy: 0.9494 - val_loss: 0.4673 - val_accuracy: 0.8504\n",
            "Epoch 125/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1314 - accuracy: 0.9464 - val_loss: 0.4913 - val_accuracy: 0.8483\n",
            "Epoch 126/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9566 - val_loss: 0.4651 - val_accuracy: 0.8515\n",
            "Epoch 127/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1242 - accuracy: 0.9515 - val_loss: 0.5426 - val_accuracy: 0.8403\n",
            "Epoch 128/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.9506 - val_loss: 0.4865 - val_accuracy: 0.8563\n",
            "Epoch 129/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1180 - accuracy: 0.9542 - val_loss: 0.4479 - val_accuracy: 0.8590\n",
            "Epoch 130/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1286 - accuracy: 0.9499 - val_loss: 0.4955 - val_accuracy: 0.8451\n",
            "Epoch 131/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1221 - accuracy: 0.9516 - val_loss: 0.4625 - val_accuracy: 0.8622\n",
            "Epoch 132/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1208 - accuracy: 0.9515 - val_loss: 0.4530 - val_accuracy: 0.8552\n",
            "Epoch 133/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1210 - accuracy: 0.9502 - val_loss: 0.4733 - val_accuracy: 0.8483\n",
            "Epoch 134/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.9551 - val_loss: 0.4641 - val_accuracy: 0.8483\n",
            "Epoch 135/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.9566 - val_loss: 0.4914 - val_accuracy: 0.8339\n",
            "Epoch 136/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9539 - val_loss: 0.4890 - val_accuracy: 0.8568\n",
            "Epoch 137/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.9534 - val_loss: 0.4691 - val_accuracy: 0.8536\n",
            "Epoch 138/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1145 - accuracy: 0.9531 - val_loss: 0.4682 - val_accuracy: 0.8616\n",
            "Epoch 139/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1117 - accuracy: 0.9559 - val_loss: 0.4615 - val_accuracy: 0.8563\n",
            "Epoch 140/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.9567 - val_loss: 0.5421 - val_accuracy: 0.8462\n",
            "Epoch 141/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.9520 - val_loss: 0.5102 - val_accuracy: 0.8462\n",
            "Epoch 142/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.9556 - val_loss: 0.5098 - val_accuracy: 0.8536\n",
            "Epoch 143/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1068 - accuracy: 0.9568 - val_loss: 0.4982 - val_accuracy: 0.8456\n",
            "Epoch 144/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1112 - accuracy: 0.9563 - val_loss: 0.5316 - val_accuracy: 0.8424\n",
            "Epoch 145/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1098 - accuracy: 0.9555 - val_loss: 0.4693 - val_accuracy: 0.8558\n",
            "Epoch 146/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1011 - accuracy: 0.9591 - val_loss: 0.5340 - val_accuracy: 0.8547\n",
            "Epoch 147/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1084 - accuracy: 0.9579 - val_loss: 0.5200 - val_accuracy: 0.8446\n",
            "Epoch 148/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.9567 - val_loss: 0.5005 - val_accuracy: 0.8483\n",
            "Epoch 149/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0999 - accuracy: 0.9585 - val_loss: 0.4889 - val_accuracy: 0.8547\n",
            "Epoch 150/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1005 - accuracy: 0.9618 - val_loss: 0.4978 - val_accuracy: 0.8494\n",
            "Epoch 151/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1018 - accuracy: 0.9603 - val_loss: 0.5391 - val_accuracy: 0.8381\n",
            "Epoch 152/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1058 - accuracy: 0.9593 - val_loss: 0.5353 - val_accuracy: 0.8504\n",
            "Epoch 153/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1095 - accuracy: 0.9579 - val_loss: 0.4987 - val_accuracy: 0.8547\n",
            "Epoch 154/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0987 - accuracy: 0.9630 - val_loss: 0.5094 - val_accuracy: 0.8510\n",
            "Epoch 155/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1007 - accuracy: 0.9625 - val_loss: 0.5226 - val_accuracy: 0.8483\n",
            "Epoch 156/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0945 - accuracy: 0.9630 - val_loss: 0.5046 - val_accuracy: 0.8568\n",
            "Epoch 157/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0985 - accuracy: 0.9602 - val_loss: 0.5490 - val_accuracy: 0.8456\n",
            "Epoch 158/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1039 - accuracy: 0.9601 - val_loss: 0.5394 - val_accuracy: 0.8558\n",
            "Epoch 159/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0987 - accuracy: 0.9611 - val_loss: 0.5984 - val_accuracy: 0.8424\n",
            "Epoch 160/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1015 - accuracy: 0.9605 - val_loss: 0.5703 - val_accuracy: 0.8387\n",
            "Epoch 161/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.1015 - accuracy: 0.9610 - val_loss: 0.5668 - val_accuracy: 0.8360\n",
            "Epoch 162/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0953 - accuracy: 0.9643 - val_loss: 0.5161 - val_accuracy: 0.8547\n",
            "Epoch 163/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0898 - accuracy: 0.9650 - val_loss: 0.5148 - val_accuracy: 0.8584\n",
            "Epoch 164/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0913 - accuracy: 0.9663 - val_loss: 0.5393 - val_accuracy: 0.8558\n",
            "Epoch 165/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0978 - accuracy: 0.9629 - val_loss: 0.5362 - val_accuracy: 0.8494\n",
            "Epoch 166/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0949 - accuracy: 0.9621 - val_loss: 0.5100 - val_accuracy: 0.8568\n",
            "Epoch 167/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0949 - accuracy: 0.9631 - val_loss: 0.5558 - val_accuracy: 0.8515\n",
            "Epoch 168/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.5275 - val_accuracy: 0.8520\n",
            "Epoch 169/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0897 - accuracy: 0.9675 - val_loss: 0.5285 - val_accuracy: 0.8595\n",
            "Epoch 170/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0909 - accuracy: 0.9654 - val_loss: 0.5300 - val_accuracy: 0.8510\n",
            "Epoch 171/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0882 - accuracy: 0.9661 - val_loss: 0.5663 - val_accuracy: 0.8456\n",
            "Epoch 172/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0853 - accuracy: 0.9678 - val_loss: 0.5622 - val_accuracy: 0.8510\n",
            "Epoch 173/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0909 - accuracy: 0.9650 - val_loss: 0.5380 - val_accuracy: 0.8542\n",
            "Epoch 174/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0930 - accuracy: 0.9646 - val_loss: 0.5466 - val_accuracy: 0.8622\n",
            "Epoch 175/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0816 - accuracy: 0.9702 - val_loss: 0.5182 - val_accuracy: 0.8547\n",
            "Epoch 176/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0898 - accuracy: 0.9642 - val_loss: 0.5403 - val_accuracy: 0.8520\n",
            "Epoch 177/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0878 - accuracy: 0.9673 - val_loss: 0.5225 - val_accuracy: 0.8504\n",
            "Epoch 178/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0778 - accuracy: 0.9701 - val_loss: 0.5331 - val_accuracy: 0.8515\n",
            "Epoch 179/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0814 - accuracy: 0.9698 - val_loss: 0.5643 - val_accuracy: 0.8675\n",
            "Epoch 180/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0969 - accuracy: 0.9618 - val_loss: 0.5542 - val_accuracy: 0.8531\n",
            "Epoch 181/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0745 - accuracy: 0.9717 - val_loss: 0.5695 - val_accuracy: 0.8542\n",
            "Epoch 182/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0956 - accuracy: 0.9665 - val_loss: 0.5699 - val_accuracy: 0.8435\n",
            "Epoch 183/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0793 - accuracy: 0.9687 - val_loss: 0.5478 - val_accuracy: 0.8606\n",
            "Epoch 184/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0792 - accuracy: 0.9718 - val_loss: 0.5478 - val_accuracy: 0.8616\n",
            "Epoch 185/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0893 - accuracy: 0.9695 - val_loss: 0.5985 - val_accuracy: 0.8429\n",
            "Epoch 186/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0826 - accuracy: 0.9713 - val_loss: 0.5907 - val_accuracy: 0.8510\n",
            "Epoch 187/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0721 - accuracy: 0.9741 - val_loss: 0.6212 - val_accuracy: 0.8467\n",
            "Epoch 188/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0831 - accuracy: 0.9693 - val_loss: 0.5810 - val_accuracy: 0.8456\n",
            "Epoch 189/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0881 - accuracy: 0.9682 - val_loss: 0.5949 - val_accuracy: 0.8520\n",
            "Epoch 190/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0801 - accuracy: 0.9711 - val_loss: 0.5968 - val_accuracy: 0.8456\n",
            "Epoch 191/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0713 - accuracy: 0.9738 - val_loss: 0.5616 - val_accuracy: 0.8542\n",
            "Epoch 192/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0860 - accuracy: 0.9683 - val_loss: 0.5904 - val_accuracy: 0.8590\n",
            "Epoch 193/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0782 - accuracy: 0.9701 - val_loss: 0.5901 - val_accuracy: 0.8531\n",
            "Epoch 194/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0810 - accuracy: 0.9699 - val_loss: 0.6268 - val_accuracy: 0.8483\n",
            "Epoch 195/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0858 - accuracy: 0.9691 - val_loss: 0.6118 - val_accuracy: 0.8531\n",
            "Epoch 196/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0755 - accuracy: 0.9734 - val_loss: 0.6338 - val_accuracy: 0.8563\n",
            "Epoch 197/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0722 - accuracy: 0.9725 - val_loss: 0.5883 - val_accuracy: 0.8542\n",
            "Epoch 198/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0689 - accuracy: 0.9729 - val_loss: 0.5657 - val_accuracy: 0.8600\n",
            "Epoch 199/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0735 - accuracy: 0.9746 - val_loss: 0.5863 - val_accuracy: 0.8563\n",
            "Epoch 200/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0802 - accuracy: 0.9690 - val_loss: 0.5867 - val_accuracy: 0.8510\n",
            "Epoch 201/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0778 - accuracy: 0.9735 - val_loss: 0.6094 - val_accuracy: 0.8515\n",
            "Epoch 202/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0622 - accuracy: 0.9780 - val_loss: 0.6113 - val_accuracy: 0.8392\n",
            "Epoch 203/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0863 - accuracy: 0.9681 - val_loss: 0.6299 - val_accuracy: 0.8494\n",
            "Epoch 204/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0727 - accuracy: 0.9734 - val_loss: 0.6971 - val_accuracy: 0.8376\n",
            "Epoch 205/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0795 - accuracy: 0.9683 - val_loss: 0.5896 - val_accuracy: 0.8526\n",
            "Epoch 206/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0728 - accuracy: 0.9737 - val_loss: 0.6856 - val_accuracy: 0.8333\n",
            "Epoch 207/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0715 - accuracy: 0.9729 - val_loss: 0.6542 - val_accuracy: 0.8504\n",
            "Epoch 208/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0705 - accuracy: 0.9725 - val_loss: 0.6121 - val_accuracy: 0.8520\n",
            "Epoch 209/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0708 - accuracy: 0.9739 - val_loss: 0.6244 - val_accuracy: 0.8590\n",
            "Epoch 210/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0722 - accuracy: 0.9733 - val_loss: 0.6501 - val_accuracy: 0.8504\n",
            "Epoch 211/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0585 - accuracy: 0.9792 - val_loss: 0.6483 - val_accuracy: 0.8515\n",
            "Epoch 212/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0779 - accuracy: 0.9734 - val_loss: 0.6584 - val_accuracy: 0.8515\n",
            "Epoch 213/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0760 - accuracy: 0.9691 - val_loss: 0.6672 - val_accuracy: 0.8515\n",
            "Epoch 214/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0693 - accuracy: 0.9746 - val_loss: 0.6922 - val_accuracy: 0.8365\n",
            "Epoch 215/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0651 - accuracy: 0.9769 - val_loss: 0.6288 - val_accuracy: 0.8446\n",
            "Epoch 216/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0646 - accuracy: 0.9764 - val_loss: 0.5938 - val_accuracy: 0.8552\n",
            "Epoch 217/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0653 - accuracy: 0.9749 - val_loss: 0.6467 - val_accuracy: 0.8563\n",
            "Epoch 218/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0658 - accuracy: 0.9764 - val_loss: 0.6993 - val_accuracy: 0.8478\n",
            "Epoch 219/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0779 - accuracy: 0.9722 - val_loss: 0.7123 - val_accuracy: 0.8515\n",
            "Epoch 220/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0621 - accuracy: 0.9785 - val_loss: 0.6654 - val_accuracy: 0.8547\n",
            "Epoch 221/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0676 - accuracy: 0.9752 - val_loss: 0.6853 - val_accuracy: 0.8526\n",
            "Epoch 222/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.9758 - val_loss: 0.6903 - val_accuracy: 0.8494\n",
            "Epoch 223/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0695 - accuracy: 0.9749 - val_loss: 0.6708 - val_accuracy: 0.8536\n",
            "Epoch 224/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0728 - accuracy: 0.9714 - val_loss: 0.6406 - val_accuracy: 0.8552\n",
            "Epoch 225/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0477 - accuracy: 0.9849 - val_loss: 0.6578 - val_accuracy: 0.8563\n",
            "Epoch 226/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0846 - accuracy: 0.9671 - val_loss: 0.6723 - val_accuracy: 0.8510\n",
            "Epoch 227/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.9754 - val_loss: 0.6518 - val_accuracy: 0.8600\n",
            "Epoch 228/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 0.6746 - val_accuracy: 0.8488\n",
            "Epoch 229/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0719 - accuracy: 0.9761 - val_loss: 0.6434 - val_accuracy: 0.8536\n",
            "Epoch 230/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0602 - accuracy: 0.9770 - val_loss: 0.6539 - val_accuracy: 0.8558\n",
            "Epoch 231/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.6636 - val_accuracy: 0.8510\n",
            "Epoch 232/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0744 - accuracy: 0.9727 - val_loss: 0.7125 - val_accuracy: 0.8536\n",
            "Epoch 233/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0646 - accuracy: 0.9753 - val_loss: 0.6818 - val_accuracy: 0.8547\n",
            "Epoch 234/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0626 - accuracy: 0.9769 - val_loss: 0.6858 - val_accuracy: 0.8600\n",
            "Epoch 235/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0538 - accuracy: 0.9801 - val_loss: 0.6654 - val_accuracy: 0.8542\n",
            "Epoch 236/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0644 - accuracy: 0.9754 - val_loss: 0.7220 - val_accuracy: 0.8494\n",
            "Epoch 237/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.6518 - val_accuracy: 0.8515\n",
            "Epoch 238/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0642 - accuracy: 0.9774 - val_loss: 0.6429 - val_accuracy: 0.8520\n",
            "Epoch 239/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0583 - accuracy: 0.9789 - val_loss: 0.6925 - val_accuracy: 0.8558\n",
            "Epoch 240/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0648 - accuracy: 0.9786 - val_loss: 0.7205 - val_accuracy: 0.8478\n",
            "Epoch 241/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0630 - accuracy: 0.9778 - val_loss: 0.6862 - val_accuracy: 0.8584\n",
            "Epoch 242/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0664 - accuracy: 0.9765 - val_loss: 0.6685 - val_accuracy: 0.8536\n",
            "Epoch 243/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9840 - val_loss: 0.6766 - val_accuracy: 0.8552\n",
            "Epoch 244/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0612 - accuracy: 0.9786 - val_loss: 0.6512 - val_accuracy: 0.8563\n",
            "Epoch 245/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0564 - accuracy: 0.9800 - val_loss: 0.6630 - val_accuracy: 0.8611\n",
            "Epoch 246/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0605 - accuracy: 0.9794 - val_loss: 0.7269 - val_accuracy: 0.8515\n",
            "Epoch 247/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0564 - accuracy: 0.9785 - val_loss: 0.7062 - val_accuracy: 0.8547\n",
            "Epoch 248/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0513 - accuracy: 0.9812 - val_loss: 0.6847 - val_accuracy: 0.8419\n",
            "Epoch 249/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0632 - accuracy: 0.9760 - val_loss: 0.7063 - val_accuracy: 0.8520\n",
            "Epoch 250/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0587 - accuracy: 0.9789 - val_loss: 0.7252 - val_accuracy: 0.8472\n",
            "Epoch 251/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0576 - accuracy: 0.9822 - val_loss: 0.7153 - val_accuracy: 0.8488\n",
            "Epoch 252/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0692 - accuracy: 0.9773 - val_loss: 0.7212 - val_accuracy: 0.8478\n",
            "Epoch 253/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0592 - accuracy: 0.9809 - val_loss: 0.7115 - val_accuracy: 0.8552\n",
            "Epoch 254/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0574 - accuracy: 0.9802 - val_loss: 0.7269 - val_accuracy: 0.8520\n",
            "Epoch 255/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0511 - accuracy: 0.9829 - val_loss: 0.7211 - val_accuracy: 0.8558\n",
            "Epoch 256/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0556 - accuracy: 0.9798 - val_loss: 0.6960 - val_accuracy: 0.8424\n",
            "Epoch 257/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0668 - accuracy: 0.9757 - val_loss: 0.7250 - val_accuracy: 0.8520\n",
            "Epoch 258/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0512 - accuracy: 0.9806 - val_loss: 0.6968 - val_accuracy: 0.8568\n",
            "Epoch 259/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0686 - accuracy: 0.9761 - val_loss: 0.6903 - val_accuracy: 0.8547\n",
            "Epoch 260/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0525 - accuracy: 0.9816 - val_loss: 0.6900 - val_accuracy: 0.8547\n",
            "Epoch 261/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0553 - accuracy: 0.9821 - val_loss: 0.7429 - val_accuracy: 0.8488\n",
            "Epoch 262/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0670 - accuracy: 0.9790 - val_loss: 0.7317 - val_accuracy: 0.8504\n",
            "Epoch 263/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0528 - accuracy: 0.9809 - val_loss: 0.7277 - val_accuracy: 0.8568\n",
            "Epoch 264/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0647 - accuracy: 0.9774 - val_loss: 0.7188 - val_accuracy: 0.8579\n",
            "Epoch 265/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0463 - accuracy: 0.9846 - val_loss: 0.7158 - val_accuracy: 0.8574\n",
            "Epoch 266/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0565 - accuracy: 0.9817 - val_loss: 0.7406 - val_accuracy: 0.8606\n",
            "Epoch 267/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0598 - accuracy: 0.9801 - val_loss: 0.7761 - val_accuracy: 0.8456\n",
            "Epoch 268/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.6546 - val_accuracy: 0.8659\n",
            "Epoch 269/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0552 - accuracy: 0.9801 - val_loss: 0.7105 - val_accuracy: 0.8536\n",
            "Epoch 270/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.7163 - val_accuracy: 0.8622\n",
            "Epoch 271/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0523 - accuracy: 0.9809 - val_loss: 0.6839 - val_accuracy: 0.8579\n",
            "Epoch 272/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 0.7618 - val_accuracy: 0.8440\n",
            "Epoch 273/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0563 - accuracy: 0.9800 - val_loss: 0.7331 - val_accuracy: 0.8499\n",
            "Epoch 274/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0611 - accuracy: 0.9778 - val_loss: 0.7257 - val_accuracy: 0.8542\n",
            "Epoch 275/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0449 - accuracy: 0.9848 - val_loss: 0.7394 - val_accuracy: 0.8542\n",
            "Epoch 276/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0422 - accuracy: 0.9861 - val_loss: 0.7964 - val_accuracy: 0.8526\n",
            "Epoch 277/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0707 - accuracy: 0.9766 - val_loss: 0.7812 - val_accuracy: 0.8381\n",
            "Epoch 278/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0472 - accuracy: 0.9838 - val_loss: 0.7330 - val_accuracy: 0.8606\n",
            "Epoch 279/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0460 - accuracy: 0.9830 - val_loss: 0.7873 - val_accuracy: 0.8579\n",
            "Epoch 280/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0672 - accuracy: 0.9769 - val_loss: 0.7596 - val_accuracy: 0.8595\n",
            "Epoch 281/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.7310 - val_accuracy: 0.8531\n",
            "Epoch 282/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0439 - accuracy: 0.9868 - val_loss: 0.7860 - val_accuracy: 0.8510\n",
            "Epoch 283/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0716 - accuracy: 0.9780 - val_loss: 0.7433 - val_accuracy: 0.8563\n",
            "Epoch 284/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0418 - accuracy: 0.9866 - val_loss: 0.7175 - val_accuracy: 0.8542\n",
            "Epoch 285/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0404 - accuracy: 0.9889 - val_loss: 0.7116 - val_accuracy: 0.8584\n",
            "Epoch 286/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0447 - accuracy: 0.9852 - val_loss: 0.7386 - val_accuracy: 0.8531\n",
            "Epoch 287/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.7556 - val_accuracy: 0.8526\n",
            "Epoch 288/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0538 - accuracy: 0.9804 - val_loss: 0.7543 - val_accuracy: 0.8595\n",
            "Epoch 289/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0536 - accuracy: 0.9798 - val_loss: 0.7336 - val_accuracy: 0.8606\n",
            "Epoch 290/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.9822 - val_loss: 0.7488 - val_accuracy: 0.8536\n",
            "Epoch 291/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 0.7780 - val_accuracy: 0.8531\n",
            "Epoch 292/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0718 - accuracy: 0.9752 - val_loss: 0.7445 - val_accuracy: 0.8574\n",
            "Epoch 293/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0422 - accuracy: 0.9852 - val_loss: 0.7517 - val_accuracy: 0.8606\n",
            "Epoch 294/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0400 - accuracy: 0.9856 - val_loss: 0.7574 - val_accuracy: 0.8649\n",
            "Epoch 295/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0482 - accuracy: 0.9845 - val_loss: 0.7635 - val_accuracy: 0.8568\n",
            "Epoch 296/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0493 - accuracy: 0.9828 - val_loss: 0.7490 - val_accuracy: 0.8584\n",
            "Epoch 297/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.7564 - val_accuracy: 0.8563\n",
            "Epoch 298/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9828 - val_loss: 0.7890 - val_accuracy: 0.8574\n",
            "Epoch 299/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0708 - accuracy: 0.9768 - val_loss: 0.7821 - val_accuracy: 0.8542\n",
            "Epoch 300/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0349 - accuracy: 0.9890 - val_loss: 0.7519 - val_accuracy: 0.8665\n",
            "Epoch 301/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.7448 - val_accuracy: 0.8665\n",
            "Epoch 302/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0422 - accuracy: 0.9865 - val_loss: 0.7988 - val_accuracy: 0.8510\n",
            "Epoch 303/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0808 - accuracy: 0.9713 - val_loss: 0.7999 - val_accuracy: 0.8590\n",
            "Epoch 304/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0375 - accuracy: 0.9882 - val_loss: 0.7586 - val_accuracy: 0.8568\n",
            "Epoch 305/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0433 - accuracy: 0.9864 - val_loss: 0.8561 - val_accuracy: 0.8600\n",
            "Epoch 306/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0537 - accuracy: 0.9810 - val_loss: 0.8753 - val_accuracy: 0.8424\n",
            "Epoch 307/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0534 - accuracy: 0.9832 - val_loss: 0.8612 - val_accuracy: 0.8590\n",
            "Epoch 308/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0443 - accuracy: 0.9854 - val_loss: 0.8099 - val_accuracy: 0.8547\n",
            "Epoch 309/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0477 - accuracy: 0.9841 - val_loss: 0.7736 - val_accuracy: 0.8563\n",
            "Epoch 310/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.7776 - val_accuracy: 0.8568\n",
            "Epoch 311/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0414 - accuracy: 0.9860 - val_loss: 0.8809 - val_accuracy: 0.8542\n",
            "Epoch 312/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0665 - accuracy: 0.9762 - val_loss: 0.8115 - val_accuracy: 0.8456\n",
            "Epoch 313/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 0.8279 - val_accuracy: 0.8494\n",
            "Epoch 314/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0382 - accuracy: 0.9869 - val_loss: 0.7791 - val_accuracy: 0.8568\n",
            "Epoch 315/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0431 - accuracy: 0.9861 - val_loss: 0.9520 - val_accuracy: 0.8376\n",
            "Epoch 316/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0642 - accuracy: 0.9776 - val_loss: 0.7764 - val_accuracy: 0.8622\n",
            "Epoch 317/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0411 - accuracy: 0.9868 - val_loss: 0.7901 - val_accuracy: 0.8574\n",
            "Epoch 318/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 0.8455 - val_accuracy: 0.8494\n",
            "Epoch 319/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.7910 - val_accuracy: 0.8510\n",
            "Epoch 320/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0666 - accuracy: 0.9782 - val_loss: 0.8519 - val_accuracy: 0.8429\n",
            "Epoch 321/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 0.8432 - val_accuracy: 0.8536\n",
            "Epoch 322/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9889 - val_loss: 0.7600 - val_accuracy: 0.8622\n",
            "Epoch 323/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0282 - accuracy: 0.9920 - val_loss: 0.8398 - val_accuracy: 0.8563\n",
            "Epoch 324/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0670 - accuracy: 0.9770 - val_loss: 0.8611 - val_accuracy: 0.8531\n",
            "Epoch 325/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0451 - accuracy: 0.9853 - val_loss: 0.7998 - val_accuracy: 0.8510\n",
            "Epoch 326/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0429 - accuracy: 0.9852 - val_loss: 0.8685 - val_accuracy: 0.8483\n",
            "Epoch 327/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0543 - accuracy: 0.9828 - val_loss: 0.7789 - val_accuracy: 0.8649\n",
            "Epoch 328/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.8033 - val_accuracy: 0.8584\n",
            "Epoch 329/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.9832 - val_loss: 0.8105 - val_accuracy: 0.8542\n",
            "Epoch 330/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0247 - accuracy: 0.9932 - val_loss: 0.8668 - val_accuracy: 0.8413\n",
            "Epoch 331/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9813 - val_loss: 0.8293 - val_accuracy: 0.8568\n",
            "Epoch 332/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0618 - accuracy: 0.9789 - val_loss: 0.8685 - val_accuracy: 0.8435\n",
            "Epoch 333/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0300 - accuracy: 0.9900 - val_loss: 0.8364 - val_accuracy: 0.8499\n",
            "Epoch 334/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0441 - accuracy: 0.9844 - val_loss: 0.8796 - val_accuracy: 0.8536\n",
            "Epoch 335/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0395 - accuracy: 0.9880 - val_loss: 0.8606 - val_accuracy: 0.8403\n",
            "Epoch 336/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0469 - accuracy: 0.9854 - val_loss: 0.8113 - val_accuracy: 0.8520\n",
            "Epoch 337/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0560 - accuracy: 0.9810 - val_loss: 0.7939 - val_accuracy: 0.8632\n",
            "Epoch 338/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0386 - accuracy: 0.9869 - val_loss: 0.8146 - val_accuracy: 0.8584\n",
            "Epoch 339/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 0.7998 - val_accuracy: 0.8616\n",
            "Epoch 340/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0374 - accuracy: 0.9878 - val_loss: 0.8044 - val_accuracy: 0.8600\n",
            "Epoch 341/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0446 - accuracy: 0.9853 - val_loss: 0.8519 - val_accuracy: 0.8446\n",
            "Epoch 342/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0304 - accuracy: 0.9906 - val_loss: 0.7667 - val_accuracy: 0.8584\n",
            "Epoch 343/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0693 - accuracy: 0.9774 - val_loss: 0.8357 - val_accuracy: 0.8590\n",
            "Epoch 344/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0390 - accuracy: 0.9872 - val_loss: 0.7716 - val_accuracy: 0.8563\n",
            "Epoch 345/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0339 - accuracy: 0.9884 - val_loss: 0.8261 - val_accuracy: 0.8536\n",
            "Epoch 346/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0372 - accuracy: 0.9870 - val_loss: 0.8421 - val_accuracy: 0.8584\n",
            "Epoch 347/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0603 - accuracy: 0.9818 - val_loss: 0.8890 - val_accuracy: 0.8510\n",
            "Epoch 348/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0430 - accuracy: 0.9848 - val_loss: 0.8536 - val_accuracy: 0.8478\n",
            "Epoch 349/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 0.8402 - val_accuracy: 0.8649\n",
            "Epoch 350/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9837 - val_loss: 0.9572 - val_accuracy: 0.8510\n",
            "Epoch 351/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0620 - accuracy: 0.9809 - val_loss: 0.8643 - val_accuracy: 0.8659\n",
            "Epoch 352/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.8312 - val_accuracy: 0.8606\n",
            "Epoch 353/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0390 - accuracy: 0.9865 - val_loss: 0.9301 - val_accuracy: 0.8547\n",
            "Epoch 354/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9817 - val_loss: 0.8290 - val_accuracy: 0.8542\n",
            "Epoch 355/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0302 - accuracy: 0.9898 - val_loss: 0.8140 - val_accuracy: 0.8595\n",
            "Epoch 356/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0387 - accuracy: 0.9876 - val_loss: 0.8236 - val_accuracy: 0.8542\n",
            "Epoch 357/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.9774 - val_loss: 0.9153 - val_accuracy: 0.8558\n",
            "Epoch 358/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0408 - accuracy: 0.9874 - val_loss: 0.8276 - val_accuracy: 0.8552\n",
            "Epoch 359/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0265 - accuracy: 0.9906 - val_loss: 0.8272 - val_accuracy: 0.8574\n",
            "Epoch 360/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0446 - accuracy: 0.9858 - val_loss: 0.8958 - val_accuracy: 0.8600\n",
            "Epoch 361/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0531 - accuracy: 0.9826 - val_loss: 0.8620 - val_accuracy: 0.8606\n",
            "Epoch 362/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 0.8454 - val_accuracy: 0.8584\n",
            "Epoch 363/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0287 - accuracy: 0.9913 - val_loss: 0.8298 - val_accuracy: 0.8494\n",
            "Epoch 364/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0485 - accuracy: 0.9841 - val_loss: 0.9225 - val_accuracy: 0.8611\n",
            "Epoch 365/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0434 - accuracy: 0.9852 - val_loss: 0.8866 - val_accuracy: 0.8531\n",
            "Epoch 366/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0561 - accuracy: 0.9813 - val_loss: 0.8472 - val_accuracy: 0.8611\n",
            "Epoch 367/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 0.9143 - val_accuracy: 0.8547\n",
            "Epoch 368/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.8293 - val_accuracy: 0.8574\n",
            "Epoch 369/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9878 - val_loss: 0.8753 - val_accuracy: 0.8462\n",
            "Epoch 370/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0558 - accuracy: 0.9794 - val_loss: 0.8868 - val_accuracy: 0.8547\n",
            "Epoch 371/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9892 - val_loss: 0.8074 - val_accuracy: 0.8590\n",
            "Epoch 372/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.8497 - val_accuracy: 0.8542\n",
            "Epoch 373/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9893 - val_loss: 0.8285 - val_accuracy: 0.8520\n",
            "Epoch 374/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9852 - val_loss: 0.9023 - val_accuracy: 0.8531\n",
            "Epoch 375/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.8273 - val_accuracy: 0.8590\n",
            "Epoch 376/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0486 - accuracy: 0.9830 - val_loss: 0.9078 - val_accuracy: 0.8542\n",
            "Epoch 377/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0389 - accuracy: 0.9861 - val_loss: 0.8627 - val_accuracy: 0.8510\n",
            "Epoch 378/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0341 - accuracy: 0.9888 - val_loss: 0.9439 - val_accuracy: 0.8462\n",
            "Epoch 379/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0607 - accuracy: 0.9801 - val_loss: 0.9181 - val_accuracy: 0.8584\n",
            "Epoch 380/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0297 - accuracy: 0.9900 - val_loss: 0.8240 - val_accuracy: 0.8611\n",
            "Epoch 381/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0349 - accuracy: 0.9917 - val_loss: 1.0109 - val_accuracy: 0.8552\n",
            "Epoch 382/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0603 - accuracy: 0.9812 - val_loss: 0.8888 - val_accuracy: 0.8574\n",
            "Epoch 383/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 0.8682 - val_accuracy: 0.8606\n",
            "Epoch 384/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.9457 - val_accuracy: 0.8595\n",
            "Epoch 385/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9842 - val_loss: 0.8641 - val_accuracy: 0.8574\n",
            "Epoch 386/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0296 - accuracy: 0.9917 - val_loss: 0.8821 - val_accuracy: 0.8622\n",
            "Epoch 387/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.9888 - val_accuracy: 0.8478\n",
            "Epoch 388/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0404 - accuracy: 0.9858 - val_loss: 0.9740 - val_accuracy: 0.8478\n",
            "Epoch 389/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.9709 - val_accuracy: 0.8510\n",
            "Epoch 390/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.8937 - val_accuracy: 0.8552\n",
            "Epoch 391/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9912 - val_loss: 0.9143 - val_accuracy: 0.8552\n",
            "Epoch 392/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0344 - accuracy: 0.9888 - val_loss: 0.8896 - val_accuracy: 0.8632\n",
            "Epoch 393/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0666 - accuracy: 0.9796 - val_loss: 0.9299 - val_accuracy: 0.8478\n",
            "Epoch 394/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.8854 - val_accuracy: 0.8595\n",
            "Epoch 395/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0215 - accuracy: 0.9935 - val_loss: 0.8994 - val_accuracy: 0.8579\n",
            "Epoch 396/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0327 - accuracy: 0.9892 - val_loss: 0.9026 - val_accuracy: 0.8462\n",
            "Epoch 397/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0383 - accuracy: 0.9881 - val_loss: 0.9880 - val_accuracy: 0.8568\n",
            "Epoch 398/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0508 - accuracy: 0.9820 - val_loss: 0.8457 - val_accuracy: 0.8510\n",
            "Epoch 399/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.8686 - val_accuracy: 0.8616\n",
            "Epoch 400/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.8870 - val_accuracy: 0.8568\n",
            "Epoch 401/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.9227 - val_accuracy: 0.8606\n",
            "Epoch 402/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0344 - accuracy: 0.9893 - val_loss: 0.9007 - val_accuracy: 0.8590\n",
            "Epoch 403/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0396 - accuracy: 0.9850 - val_loss: 0.9341 - val_accuracy: 0.8574\n",
            "Epoch 404/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0376 - accuracy: 0.9885 - val_loss: 0.8980 - val_accuracy: 0.8536\n",
            "Epoch 405/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.9128 - val_accuracy: 0.8574\n",
            "Epoch 406/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0285 - accuracy: 0.9906 - val_loss: 0.9572 - val_accuracy: 0.8536\n",
            "Epoch 407/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0547 - accuracy: 0.9841 - val_loss: 1.0904 - val_accuracy: 0.8429\n",
            "Epoch 408/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0542 - accuracy: 0.9837 - val_loss: 0.9082 - val_accuracy: 0.8510\n",
            "Epoch 409/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0257 - accuracy: 0.9914 - val_loss: 0.8971 - val_accuracy: 0.8584\n",
            "Epoch 410/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9916 - val_loss: 0.9032 - val_accuracy: 0.8547\n",
            "Epoch 411/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.9246 - val_accuracy: 0.8600\n",
            "Epoch 412/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0475 - accuracy: 0.9838 - val_loss: 0.9068 - val_accuracy: 0.8584\n",
            "Epoch 413/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0301 - accuracy: 0.9925 - val_loss: 0.9109 - val_accuracy: 0.8681\n",
            "Epoch 414/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0365 - accuracy: 0.9898 - val_loss: 0.9178 - val_accuracy: 0.8510\n",
            "Epoch 415/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0488 - accuracy: 0.9865 - val_loss: 0.9407 - val_accuracy: 0.8536\n",
            "Epoch 416/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.8933 - val_accuracy: 0.8574\n",
            "Epoch 417/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0299 - accuracy: 0.9894 - val_loss: 0.9313 - val_accuracy: 0.8574\n",
            "Epoch 418/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0278 - accuracy: 0.9902 - val_loss: 1.0411 - val_accuracy: 0.8520\n",
            "Epoch 419/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0363 - accuracy: 0.9854 - val_loss: 0.9513 - val_accuracy: 0.8568\n",
            "Epoch 420/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.9239 - val_accuracy: 0.8611\n",
            "Epoch 421/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 1.0115 - val_accuracy: 0.8510\n",
            "Epoch 422/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0283 - accuracy: 0.9927 - val_loss: 0.9334 - val_accuracy: 0.8531\n",
            "Epoch 423/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.9455 - val_accuracy: 0.8568\n",
            "Epoch 424/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0633 - accuracy: 0.9798 - val_loss: 1.0337 - val_accuracy: 0.8408\n",
            "Epoch 425/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0409 - accuracy: 0.9849 - val_loss: 0.9391 - val_accuracy: 0.8681\n",
            "Epoch 426/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0279 - accuracy: 0.9924 - val_loss: 0.9371 - val_accuracy: 0.8622\n",
            "Epoch 427/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0344 - accuracy: 0.9885 - val_loss: 0.9166 - val_accuracy: 0.8627\n",
            "Epoch 428/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0366 - accuracy: 0.9876 - val_loss: 0.9843 - val_accuracy: 0.8606\n",
            "Epoch 429/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0378 - accuracy: 0.9886 - val_loss: 1.0312 - val_accuracy: 0.8413\n",
            "Epoch 430/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.9294 - val_accuracy: 0.8568\n",
            "Epoch 431/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0387 - accuracy: 0.9897 - val_loss: 0.9414 - val_accuracy: 0.8600\n",
            "Epoch 432/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0256 - accuracy: 0.9920 - val_loss: 1.0443 - val_accuracy: 0.8526\n",
            "Epoch 433/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.8915 - val_accuracy: 0.8665\n",
            "Epoch 434/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0352 - accuracy: 0.9898 - val_loss: 0.9299 - val_accuracy: 0.8643\n",
            "Epoch 435/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0483 - accuracy: 0.9833 - val_loss: 1.0320 - val_accuracy: 0.8494\n",
            "Epoch 436/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0610 - accuracy: 0.9824 - val_loss: 0.9240 - val_accuracy: 0.8563\n",
            "Epoch 437/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.9651 - val_accuracy: 0.8584\n",
            "Epoch 438/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0639 - accuracy: 0.9794 - val_loss: 1.0174 - val_accuracy: 0.8606\n",
            "Epoch 439/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.9188 - val_accuracy: 0.8627\n",
            "Epoch 440/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0220 - accuracy: 0.9945 - val_loss: 0.9870 - val_accuracy: 0.8595\n",
            "Epoch 441/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9904 - val_loss: 0.9817 - val_accuracy: 0.8558\n",
            "Epoch 442/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0471 - accuracy: 0.9842 - val_loss: 0.9949 - val_accuracy: 0.8579\n",
            "Epoch 443/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0456 - accuracy: 0.9856 - val_loss: 0.9033 - val_accuracy: 0.8616\n",
            "Epoch 444/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 0.9653 - val_accuracy: 0.8542\n",
            "Epoch 445/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0457 - accuracy: 0.9842 - val_loss: 0.9011 - val_accuracy: 0.8558\n",
            "Epoch 446/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.9275 - val_accuracy: 0.8632\n",
            "Epoch 447/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.9855 - val_accuracy: 0.8440\n",
            "Epoch 448/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0372 - accuracy: 0.9900 - val_loss: 0.9767 - val_accuracy: 0.8649\n",
            "Epoch 449/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0426 - accuracy: 0.9873 - val_loss: 1.0123 - val_accuracy: 0.8424\n",
            "Epoch 450/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0436 - accuracy: 0.9860 - val_loss: 0.9293 - val_accuracy: 0.8606\n",
            "Epoch 451/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.8759 - val_accuracy: 0.8542\n",
            "Epoch 452/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0183 - accuracy: 0.9959 - val_loss: 0.9110 - val_accuracy: 0.8574\n",
            "Epoch 453/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0371 - accuracy: 0.9890 - val_loss: 0.9975 - val_accuracy: 0.8520\n",
            "Epoch 454/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0680 - accuracy: 0.9778 - val_loss: 0.9856 - val_accuracy: 0.8568\n",
            "Epoch 455/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.9694 - val_accuracy: 0.8595\n",
            "Epoch 456/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0267 - accuracy: 0.9921 - val_loss: 0.9704 - val_accuracy: 0.8600\n",
            "Epoch 457/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 1.0313 - val_accuracy: 0.8595\n",
            "Epoch 458/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0419 - accuracy: 0.9872 - val_loss: 0.9279 - val_accuracy: 0.8510\n",
            "Epoch 459/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.9481 - val_accuracy: 0.8558\n",
            "Epoch 460/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0476 - accuracy: 0.9857 - val_loss: 1.0669 - val_accuracy: 0.8542\n",
            "Epoch 461/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0604 - accuracy: 0.9816 - val_loss: 0.9955 - val_accuracy: 0.8472\n",
            "Epoch 462/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0356 - accuracy: 0.9900 - val_loss: 0.9769 - val_accuracy: 0.8595\n",
            "Epoch 463/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0116 - accuracy: 0.9972 - val_loss: 0.8976 - val_accuracy: 0.8606\n",
            "Epoch 464/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0360 - accuracy: 0.9892 - val_loss: 0.9978 - val_accuracy: 0.8547\n",
            "Epoch 465/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0471 - accuracy: 0.9852 - val_loss: 0.9085 - val_accuracy: 0.8600\n",
            "Epoch 466/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 0.9861 - val_accuracy: 0.8643\n",
            "Epoch 467/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9828 - val_loss: 1.0832 - val_accuracy: 0.8440\n",
            "Epoch 468/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0510 - accuracy: 0.9833 - val_loss: 0.9359 - val_accuracy: 0.8446\n",
            "Epoch 469/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9937 - val_loss: 0.9143 - val_accuracy: 0.8665\n",
            "Epoch 470/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0128 - accuracy: 0.9972 - val_loss: 0.9046 - val_accuracy: 0.8616\n",
            "Epoch 471/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0567 - accuracy: 0.9840 - val_loss: 0.9832 - val_accuracy: 0.8536\n",
            "Epoch 472/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0343 - accuracy: 0.9881 - val_loss: 0.9422 - val_accuracy: 0.8552\n",
            "Epoch 473/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0392 - accuracy: 0.9885 - val_loss: 0.9786 - val_accuracy: 0.8643\n",
            "Epoch 474/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0276 - accuracy: 0.9916 - val_loss: 1.0381 - val_accuracy: 0.8510\n",
            "Epoch 475/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0318 - accuracy: 0.9900 - val_loss: 0.9923 - val_accuracy: 0.8579\n",
            "Epoch 476/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 1.0499 - val_accuracy: 0.8563\n",
            "Epoch 477/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0306 - accuracy: 0.9916 - val_loss: 1.1812 - val_accuracy: 0.8552\n",
            "Epoch 478/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0441 - accuracy: 0.9858 - val_loss: 0.9963 - val_accuracy: 0.8600\n",
            "Epoch 479/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0270 - accuracy: 0.9919 - val_loss: 1.0017 - val_accuracy: 0.8568\n",
            "Epoch 480/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0229 - accuracy: 0.9932 - val_loss: 1.0646 - val_accuracy: 0.8568\n",
            "Epoch 481/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0236 - accuracy: 0.9933 - val_loss: 1.0372 - val_accuracy: 0.8520\n",
            "Epoch 482/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0539 - accuracy: 0.9830 - val_loss: 1.0174 - val_accuracy: 0.8675\n",
            "Epoch 483/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9928 - val_loss: 0.9276 - val_accuracy: 0.8665\n",
            "Epoch 484/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0298 - accuracy: 0.9901 - val_loss: 0.9946 - val_accuracy: 0.8606\n",
            "Epoch 485/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0335 - accuracy: 0.9889 - val_loss: 0.9485 - val_accuracy: 0.8670\n",
            "Epoch 486/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.9504 - val_accuracy: 0.8659\n",
            "Epoch 487/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0375 - accuracy: 0.9881 - val_loss: 1.0546 - val_accuracy: 0.8542\n",
            "Epoch 488/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0411 - accuracy: 0.9877 - val_loss: 1.0231 - val_accuracy: 0.8542\n",
            "Epoch 489/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0423 - accuracy: 0.9877 - val_loss: 1.0227 - val_accuracy: 0.8600\n",
            "Epoch 490/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 1.0096 - val_accuracy: 0.8600\n",
            "Epoch 491/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0384 - accuracy: 0.9881 - val_loss: 1.0570 - val_accuracy: 0.8547\n",
            "Epoch 492/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0423 - accuracy: 0.9874 - val_loss: 1.0252 - val_accuracy: 0.8558\n",
            "Epoch 493/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0389 - accuracy: 0.9901 - val_loss: 0.9554 - val_accuracy: 0.8531\n",
            "Epoch 494/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 1.0661 - val_accuracy: 0.8606\n",
            "Epoch 495/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 1.0505 - val_accuracy: 0.8584\n",
            "Epoch 496/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0328 - accuracy: 0.9908 - val_loss: 1.0167 - val_accuracy: 0.8547\n",
            "Epoch 497/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0416 - accuracy: 0.9882 - val_loss: 1.0043 - val_accuracy: 0.8579\n",
            "Epoch 498/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0229 - accuracy: 0.9929 - val_loss: 1.0496 - val_accuracy: 0.8504\n",
            "Epoch 499/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0288 - accuracy: 0.9908 - val_loss: 1.0175 - val_accuracy: 0.8542\n",
            "Epoch 500/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9846 - val_loss: 1.0120 - val_accuracy: 0.8600\n",
            "Epoch 501/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 1.0002 - val_accuracy: 0.8616\n",
            "Epoch 502/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: 1.0003 - val_accuracy: 0.8547\n",
            "Epoch 503/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 1.0252 - val_accuracy: 0.8483\n",
            "Epoch 504/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 1.0732 - val_accuracy: 0.8536\n",
            "Epoch 505/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0359 - accuracy: 0.9894 - val_loss: 0.9833 - val_accuracy: 0.8622\n",
            "Epoch 506/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0303 - accuracy: 0.9916 - val_loss: 1.0547 - val_accuracy: 0.8579\n",
            "Epoch 507/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 1.1031 - val_accuracy: 0.8446\n",
            "Epoch 508/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 1.1079 - val_accuracy: 0.8467\n",
            "Epoch 509/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0600 - accuracy: 0.9838 - val_loss: 0.9831 - val_accuracy: 0.8574\n",
            "Epoch 510/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.9904 - val_accuracy: 0.8542\n",
            "Epoch 511/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0323 - accuracy: 0.9905 - val_loss: 0.9944 - val_accuracy: 0.8515\n",
            "Epoch 512/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9913 - val_loss: 1.0654 - val_accuracy: 0.8558\n",
            "Epoch 513/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0305 - accuracy: 0.9897 - val_loss: 0.9864 - val_accuracy: 0.8670\n",
            "Epoch 514/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 0.9961 - val_accuracy: 0.8526\n",
            "Epoch 515/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0202 - accuracy: 0.9928 - val_loss: 0.9920 - val_accuracy: 0.8595\n",
            "Epoch 516/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0412 - accuracy: 0.9881 - val_loss: 1.0303 - val_accuracy: 0.8579\n",
            "Epoch 517/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0329 - accuracy: 0.9889 - val_loss: 1.0889 - val_accuracy: 0.8472\n",
            "Epoch 518/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0613 - accuracy: 0.9841 - val_loss: 1.0221 - val_accuracy: 0.8579\n",
            "Epoch 519/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.9595 - val_accuracy: 0.8681\n",
            "Epoch 520/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 0.9807 - val_accuracy: 0.8622\n",
            "Epoch 521/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0620 - accuracy: 0.9824 - val_loss: 1.0493 - val_accuracy: 0.8611\n",
            "Epoch 522/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0290 - accuracy: 0.9901 - val_loss: 1.0628 - val_accuracy: 0.8643\n",
            "Epoch 523/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0360 - accuracy: 0.9876 - val_loss: 1.0716 - val_accuracy: 0.8552\n",
            "Epoch 524/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 1.0287 - val_accuracy: 0.8611\n",
            "Epoch 525/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0396 - accuracy: 0.9896 - val_loss: 1.0407 - val_accuracy: 0.8542\n",
            "Epoch 526/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.9643 - val_accuracy: 0.8654\n",
            "Epoch 527/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 1.0094 - val_accuracy: 0.8547\n",
            "Epoch 528/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0574 - accuracy: 0.9850 - val_loss: 1.1526 - val_accuracy: 0.8552\n",
            "Epoch 529/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0317 - accuracy: 0.9913 - val_loss: 0.9930 - val_accuracy: 0.8611\n",
            "Epoch 530/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 0.9994 - val_accuracy: 0.8627\n",
            "Epoch 531/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 1.0677 - val_accuracy: 0.8472\n",
            "Epoch 532/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0487 - accuracy: 0.9850 - val_loss: 1.0751 - val_accuracy: 0.8478\n",
            "Epoch 533/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0313 - accuracy: 0.9916 - val_loss: 0.9755 - val_accuracy: 0.8622\n",
            "Epoch 534/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0124 - accuracy: 0.9957 - val_loss: 1.0088 - val_accuracy: 0.8600\n",
            "Epoch 535/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0344 - accuracy: 0.9912 - val_loss: 1.0349 - val_accuracy: 0.8584\n",
            "Epoch 536/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0584 - accuracy: 0.9822 - val_loss: 1.0767 - val_accuracy: 0.8606\n",
            "Epoch 537/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9943 - val_loss: 1.0511 - val_accuracy: 0.8574\n",
            "Epoch 538/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 1.0461 - val_accuracy: 0.8627\n",
            "Epoch 539/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.9850 - val_loss: 1.1707 - val_accuracy: 0.8413\n",
            "Epoch 540/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0295 - accuracy: 0.9902 - val_loss: 1.0196 - val_accuracy: 0.8542\n",
            "Epoch 541/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 1.0516 - val_accuracy: 0.8638\n",
            "Epoch 542/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 1.1389 - val_accuracy: 0.8451\n",
            "Epoch 543/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0347 - accuracy: 0.9889 - val_loss: 1.0216 - val_accuracy: 0.8622\n",
            "Epoch 544/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9927 - val_loss: 1.0503 - val_accuracy: 0.8526\n",
            "Epoch 545/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 1.0692 - val_accuracy: 0.8584\n",
            "Epoch 546/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0383 - accuracy: 0.9893 - val_loss: 1.0663 - val_accuracy: 0.8649\n",
            "Epoch 547/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0265 - accuracy: 0.9914 - val_loss: 1.0897 - val_accuracy: 0.8611\n",
            "Epoch 548/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 1.0933 - val_accuracy: 0.8611\n",
            "Epoch 549/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9944 - val_loss: 1.0229 - val_accuracy: 0.8600\n",
            "Epoch 550/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0176 - accuracy: 0.9947 - val_loss: 1.0900 - val_accuracy: 0.8520\n",
            "Epoch 551/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0683 - accuracy: 0.9813 - val_loss: 1.0964 - val_accuracy: 0.8547\n",
            "Epoch 552/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0248 - accuracy: 0.9925 - val_loss: 1.1294 - val_accuracy: 0.8504\n",
            "Epoch 553/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0289 - accuracy: 0.9916 - val_loss: 1.0133 - val_accuracy: 0.8600\n",
            "Epoch 554/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9963 - val_loss: 0.9913 - val_accuracy: 0.8665\n",
            "Epoch 555/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9948 - val_loss: 1.1318 - val_accuracy: 0.8542\n",
            "Epoch 556/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0820 - accuracy: 0.9761 - val_loss: 1.0105 - val_accuracy: 0.8643\n",
            "Epoch 557/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 1.0129 - val_accuracy: 0.8670\n",
            "Epoch 558/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 1.0164 - val_accuracy: 0.8611\n",
            "Epoch 559/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 1.0213 - val_accuracy: 0.8616\n",
            "Epoch 560/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 1.0747 - val_accuracy: 0.8590\n",
            "Epoch 561/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0343 - accuracy: 0.9890 - val_loss: 1.0910 - val_accuracy: 0.8584\n",
            "Epoch 562/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0293 - accuracy: 0.9923 - val_loss: 1.0670 - val_accuracy: 0.8568\n",
            "Epoch 563/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 1.1139 - val_accuracy: 0.8515\n",
            "Epoch 564/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0370 - accuracy: 0.9878 - val_loss: 1.1574 - val_accuracy: 0.8579\n",
            "Epoch 565/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 1.0828 - val_accuracy: 0.8574\n",
            "Epoch 566/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0209 - accuracy: 0.9940 - val_loss: 1.1203 - val_accuracy: 0.8611\n",
            "Epoch 567/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0748 - accuracy: 0.9797 - val_loss: 1.0980 - val_accuracy: 0.8649\n",
            "Epoch 568/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 1.0372 - val_accuracy: 0.8659\n",
            "Epoch 569/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 1.0424 - val_accuracy: 0.8606\n",
            "Epoch 570/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0275 - accuracy: 0.9920 - val_loss: 1.0772 - val_accuracy: 0.8595\n",
            "Epoch 571/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0455 - accuracy: 0.9841 - val_loss: 1.0558 - val_accuracy: 0.8568\n",
            "Epoch 572/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 1.0625 - val_accuracy: 0.8595\n",
            "Epoch 573/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9939 - val_loss: 1.1606 - val_accuracy: 0.8467\n",
            "Epoch 574/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0629 - accuracy: 0.9805 - val_loss: 1.0802 - val_accuracy: 0.8606\n",
            "Epoch 575/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 1.0350 - val_accuracy: 0.8665\n",
            "Epoch 576/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 1.0347 - val_accuracy: 0.8632\n",
            "Epoch 577/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 1.0301 - val_accuracy: 0.8686\n",
            "Epoch 578/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0302 - accuracy: 0.9902 - val_loss: 1.2840 - val_accuracy: 0.8499\n",
            "Epoch 579/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0780 - accuracy: 0.9758 - val_loss: 1.1083 - val_accuracy: 0.8467\n",
            "Epoch 580/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9919 - val_loss: 1.0395 - val_accuracy: 0.8563\n",
            "Epoch 581/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 1.0147 - val_accuracy: 0.8558\n",
            "Epoch 582/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 1.1505 - val_accuracy: 0.8462\n",
            "Epoch 583/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0408 - accuracy: 0.9880 - val_loss: 1.1513 - val_accuracy: 0.8611\n",
            "Epoch 584/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0258 - accuracy: 0.9929 - val_loss: 1.1155 - val_accuracy: 0.8467\n",
            "Epoch 585/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0425 - accuracy: 0.9861 - val_loss: 1.1082 - val_accuracy: 0.8547\n",
            "Epoch 586/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0409 - accuracy: 0.9886 - val_loss: 1.1600 - val_accuracy: 0.8606\n",
            "Epoch 587/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9921 - val_loss: 1.0874 - val_accuracy: 0.8526\n",
            "Epoch 588/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 1.0928 - val_accuracy: 0.8590\n",
            "Epoch 589/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0399 - accuracy: 0.9896 - val_loss: 1.1449 - val_accuracy: 0.8536\n",
            "Epoch 590/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0458 - accuracy: 0.9849 - val_loss: 1.1552 - val_accuracy: 0.8520\n",
            "Epoch 591/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0411 - accuracy: 0.9874 - val_loss: 1.2286 - val_accuracy: 0.8472\n",
            "Epoch 592/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 1.1168 - val_accuracy: 0.8536\n",
            "Epoch 593/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9964 - val_loss: 1.1707 - val_accuracy: 0.8499\n",
            "Epoch 594/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 1.1428 - val_accuracy: 0.8558\n",
            "Epoch 595/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0421 - accuracy: 0.9877 - val_loss: 1.1181 - val_accuracy: 0.8558\n",
            "Epoch 596/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0189 - accuracy: 0.9949 - val_loss: 1.1077 - val_accuracy: 0.8531\n",
            "Epoch 597/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9929 - val_loss: 1.1090 - val_accuracy: 0.8558\n",
            "Epoch 598/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0339 - accuracy: 0.9894 - val_loss: 1.2386 - val_accuracy: 0.8494\n",
            "Epoch 599/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0372 - accuracy: 0.9873 - val_loss: 1.1135 - val_accuracy: 0.8547\n",
            "Epoch 600/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0235 - accuracy: 0.9924 - val_loss: 1.0817 - val_accuracy: 0.8568\n",
            "Epoch 601/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0528 - accuracy: 0.9840 - val_loss: 1.0934 - val_accuracy: 0.8590\n",
            "Epoch 602/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 1.0922 - val_accuracy: 0.8632\n",
            "Epoch 603/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9906 - val_loss: 1.1351 - val_accuracy: 0.8542\n",
            "Epoch 604/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 1.2265 - val_accuracy: 0.8568\n",
            "Epoch 605/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0244 - accuracy: 0.9947 - val_loss: 1.0009 - val_accuracy: 0.8632\n",
            "Epoch 606/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 1.1609 - val_accuracy: 0.8584\n",
            "Epoch 607/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 1.0692 - val_accuracy: 0.8595\n",
            "Epoch 608/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 1.2533 - val_accuracy: 0.8478\n",
            "Epoch 609/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0740 - accuracy: 0.9798 - val_loss: 1.0708 - val_accuracy: 0.8600\n",
            "Epoch 610/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 1.0923 - val_accuracy: 0.8616\n",
            "Epoch 611/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0137 - accuracy: 0.9959 - val_loss: 1.0745 - val_accuracy: 0.8638\n",
            "Epoch 612/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0560 - accuracy: 0.9841 - val_loss: 1.1321 - val_accuracy: 0.8499\n",
            "Epoch 613/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 1.0931 - val_accuracy: 0.8574\n",
            "Epoch 614/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 1.0693 - val_accuracy: 0.8632\n",
            "Epoch 615/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 1.0691 - val_accuracy: 0.8558\n",
            "Epoch 616/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0236 - accuracy: 0.9944 - val_loss: 1.4056 - val_accuracy: 0.8467\n",
            "Epoch 617/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0669 - accuracy: 0.9801 - val_loss: 1.1087 - val_accuracy: 0.8622\n",
            "Epoch 618/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 1.0496 - val_accuracy: 0.8659\n",
            "Epoch 619/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 1.0579 - val_accuracy: 0.8697\n",
            "Epoch 620/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0332 - accuracy: 0.9912 - val_loss: 1.2608 - val_accuracy: 0.8616\n",
            "Epoch 621/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0507 - accuracy: 0.9860 - val_loss: 1.1166 - val_accuracy: 0.8558\n",
            "Epoch 622/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 1.0806 - val_accuracy: 0.8632\n",
            "Epoch 623/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0406 - accuracy: 0.9892 - val_loss: 1.2054 - val_accuracy: 0.8494\n",
            "Epoch 624/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0385 - accuracy: 0.9873 - val_loss: 1.1983 - val_accuracy: 0.8547\n",
            "Epoch 625/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0366 - accuracy: 0.9878 - val_loss: 1.1022 - val_accuracy: 0.8616\n",
            "Epoch 626/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 1.2345 - val_accuracy: 0.8574\n",
            "Epoch 627/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 1.2809 - val_accuracy: 0.8515\n",
            "Epoch 628/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 1.1480 - val_accuracy: 0.8504\n",
            "Epoch 629/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 1.1540 - val_accuracy: 0.8499\n",
            "Epoch 630/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0175 - accuracy: 0.9940 - val_loss: 1.1086 - val_accuracy: 0.8616\n",
            "Epoch 631/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0282 - accuracy: 0.9927 - val_loss: 1.1475 - val_accuracy: 0.8568\n",
            "Epoch 632/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0265 - accuracy: 0.9920 - val_loss: 1.2180 - val_accuracy: 0.8526\n",
            "Epoch 633/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0256 - accuracy: 0.9912 - val_loss: 1.1452 - val_accuracy: 0.8558\n",
            "Epoch 634/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0476 - accuracy: 0.9849 - val_loss: 1.1145 - val_accuracy: 0.8632\n",
            "Epoch 635/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0212 - accuracy: 0.9928 - val_loss: 1.2182 - val_accuracy: 0.8526\n",
            "Epoch 636/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0278 - accuracy: 0.9919 - val_loss: 1.1460 - val_accuracy: 0.8590\n",
            "Epoch 637/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0352 - accuracy: 0.9905 - val_loss: 1.1241 - val_accuracy: 0.8584\n",
            "Epoch 638/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 1.2036 - val_accuracy: 0.8574\n",
            "Epoch 639/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9913 - val_loss: 1.1437 - val_accuracy: 0.8579\n",
            "Epoch 640/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 1.1921 - val_accuracy: 0.8590\n",
            "Epoch 641/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0308 - accuracy: 0.9906 - val_loss: 1.2168 - val_accuracy: 0.8510\n",
            "Epoch 642/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0239 - accuracy: 0.9914 - val_loss: 1.1984 - val_accuracy: 0.8563\n",
            "Epoch 643/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 1.1539 - val_accuracy: 0.8574\n",
            "Epoch 644/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0199 - accuracy: 0.9951 - val_loss: 1.1175 - val_accuracy: 0.8638\n",
            "Epoch 645/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0452 - accuracy: 0.9877 - val_loss: 1.2236 - val_accuracy: 0.8574\n",
            "Epoch 646/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 1.1369 - val_accuracy: 0.8606\n",
            "Epoch 647/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0241 - accuracy: 0.9913 - val_loss: 1.1300 - val_accuracy: 0.8579\n",
            "Epoch 648/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9945 - val_loss: 1.1415 - val_accuracy: 0.8590\n",
            "Epoch 649/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9951 - val_loss: 1.1321 - val_accuracy: 0.8632\n",
            "Epoch 650/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0522 - accuracy: 0.9861 - val_loss: 1.1641 - val_accuracy: 0.8622\n",
            "Epoch 651/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0370 - accuracy: 0.9874 - val_loss: 1.1830 - val_accuracy: 0.8520\n",
            "Epoch 652/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 1.1240 - val_accuracy: 0.8536\n",
            "Epoch 653/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0330 - accuracy: 0.9889 - val_loss: 1.1653 - val_accuracy: 0.8606\n",
            "Epoch 654/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 1.1808 - val_accuracy: 0.8595\n",
            "Epoch 655/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0243 - accuracy: 0.9913 - val_loss: 1.1414 - val_accuracy: 0.8638\n",
            "Epoch 656/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 1.1972 - val_accuracy: 0.8515\n",
            "Epoch 657/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0427 - accuracy: 0.9861 - val_loss: 1.2041 - val_accuracy: 0.8563\n",
            "Epoch 658/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 1.1113 - val_accuracy: 0.8643\n",
            "Epoch 659/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0314 - accuracy: 0.9925 - val_loss: 1.1996 - val_accuracy: 0.8600\n",
            "Epoch 660/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 1.1994 - val_accuracy: 0.8542\n",
            "Epoch 661/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0363 - accuracy: 0.9888 - val_loss: 1.1612 - val_accuracy: 0.8558\n",
            "Epoch 662/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0325 - accuracy: 0.9913 - val_loss: 1.2781 - val_accuracy: 0.8446\n",
            "Epoch 663/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0323 - accuracy: 0.9901 - val_loss: 1.1043 - val_accuracy: 0.8595\n",
            "Epoch 664/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 1.1789 - val_accuracy: 0.8515\n",
            "Epoch 665/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0271 - accuracy: 0.9913 - val_loss: 1.1872 - val_accuracy: 0.8584\n",
            "Epoch 666/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0321 - accuracy: 0.9909 - val_loss: 1.1169 - val_accuracy: 0.8622\n",
            "Epoch 667/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0154 - accuracy: 0.9947 - val_loss: 1.1161 - val_accuracy: 0.8552\n",
            "Epoch 668/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0366 - accuracy: 0.9893 - val_loss: 1.1550 - val_accuracy: 0.8574\n",
            "Epoch 669/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0289 - accuracy: 0.9913 - val_loss: 1.1516 - val_accuracy: 0.8579\n",
            "Epoch 670/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0553 - accuracy: 0.9857 - val_loss: 1.1241 - val_accuracy: 0.8547\n",
            "Epoch 671/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 1.0855 - val_accuracy: 0.8643\n",
            "Epoch 672/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0123 - accuracy: 0.9975 - val_loss: 1.1274 - val_accuracy: 0.8675\n",
            "Epoch 673/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0489 - accuracy: 0.9857 - val_loss: 1.2822 - val_accuracy: 0.8435\n",
            "Epoch 674/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 1.2274 - val_accuracy: 0.8590\n",
            "Epoch 675/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 1.1294 - val_accuracy: 0.8622\n",
            "Epoch 676/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9872 - val_loss: 1.2121 - val_accuracy: 0.8584\n",
            "Epoch 677/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 1.1149 - val_accuracy: 0.8681\n",
            "Epoch 678/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9953 - val_loss: 1.1434 - val_accuracy: 0.8632\n",
            "Epoch 679/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 1.1160 - val_accuracy: 0.8665\n",
            "Epoch 680/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 1.1070 - val_accuracy: 0.8670\n",
            "Epoch 681/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0643 - accuracy: 0.9813 - val_loss: 1.2639 - val_accuracy: 0.8579\n",
            "Epoch 682/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 1.1530 - val_accuracy: 0.8584\n",
            "Epoch 683/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 1.0767 - val_accuracy: 0.8670\n",
            "Epoch 684/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 1.0790 - val_accuracy: 0.8665\n",
            "Epoch 685/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0322 - accuracy: 0.9909 - val_loss: 1.1753 - val_accuracy: 0.8552\n",
            "Epoch 686/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0291 - accuracy: 0.9912 - val_loss: 1.1389 - val_accuracy: 0.8536\n",
            "Epoch 687/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 1.1549 - val_accuracy: 0.8616\n",
            "Epoch 688/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0453 - accuracy: 0.9865 - val_loss: 1.2459 - val_accuracy: 0.8606\n",
            "Epoch 689/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0213 - accuracy: 0.9941 - val_loss: 1.1169 - val_accuracy: 0.8616\n",
            "Epoch 690/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0195 - accuracy: 0.9947 - val_loss: 1.1338 - val_accuracy: 0.8526\n",
            "Epoch 691/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0297 - accuracy: 0.9923 - val_loss: 1.1467 - val_accuracy: 0.8547\n",
            "Epoch 692/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 1.1661 - val_accuracy: 0.8659\n",
            "Epoch 693/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 1.1330 - val_accuracy: 0.8616\n",
            "Epoch 694/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 1.1269 - val_accuracy: 0.8659\n",
            "Epoch 695/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0292 - accuracy: 0.9927 - val_loss: 1.2528 - val_accuracy: 0.8558\n",
            "Epoch 696/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0637 - accuracy: 0.9828 - val_loss: 1.2768 - val_accuracy: 0.8504\n",
            "Epoch 697/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 1.1496 - val_accuracy: 0.8547\n",
            "Epoch 698/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 1.1184 - val_accuracy: 0.8600\n",
            "Epoch 699/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 1.1093 - val_accuracy: 0.8638\n",
            "Epoch 700/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0333 - accuracy: 0.9901 - val_loss: 1.1953 - val_accuracy: 0.8579\n",
            "Epoch 701/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0471 - accuracy: 0.9857 - val_loss: 1.2321 - val_accuracy: 0.8568\n",
            "Epoch 702/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 1.1666 - val_accuracy: 0.8520\n",
            "Epoch 703/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9965 - val_loss: 1.1311 - val_accuracy: 0.8590\n",
            "Epoch 704/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 1.2005 - val_accuracy: 0.8531\n",
            "Epoch 705/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.9876 - val_loss: 1.2784 - val_accuracy: 0.8558\n",
            "Epoch 706/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0305 - accuracy: 0.9913 - val_loss: 1.2287 - val_accuracy: 0.8531\n",
            "Epoch 707/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0305 - accuracy: 0.9923 - val_loss: 1.2005 - val_accuracy: 0.8590\n",
            "Epoch 708/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0247 - accuracy: 0.9913 - val_loss: 1.2065 - val_accuracy: 0.8600\n",
            "Epoch 709/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0181 - accuracy: 0.9944 - val_loss: 1.1986 - val_accuracy: 0.8616\n",
            "Epoch 710/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9929 - val_loss: 1.2220 - val_accuracy: 0.8632\n",
            "Epoch 711/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0499 - accuracy: 0.9877 - val_loss: 1.3075 - val_accuracy: 0.8494\n",
            "Epoch 712/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 1.1227 - val_accuracy: 0.8611\n",
            "Epoch 713/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 1.1434 - val_accuracy: 0.8600\n",
            "Epoch 714/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 1.2010 - val_accuracy: 0.8542\n",
            "Epoch 715/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0375 - accuracy: 0.9894 - val_loss: 1.2910 - val_accuracy: 0.8499\n",
            "Epoch 716/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0495 - accuracy: 0.9868 - val_loss: 1.1984 - val_accuracy: 0.8622\n",
            "Epoch 717/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 1.2069 - val_accuracy: 0.8563\n",
            "Epoch 718/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 1.1442 - val_accuracy: 0.8579\n",
            "Epoch 719/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0273 - accuracy: 0.9906 - val_loss: 1.2290 - val_accuracy: 0.8499\n",
            "Epoch 720/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 1.1948 - val_accuracy: 0.8579\n",
            "Epoch 721/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 1.2851 - val_accuracy: 0.8526\n",
            "Epoch 722/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0432 - accuracy: 0.9865 - val_loss: 1.2146 - val_accuracy: 0.8590\n",
            "Epoch 723/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 1.2494 - val_accuracy: 0.8638\n",
            "Epoch 724/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 1.3128 - val_accuracy: 0.8520\n",
            "Epoch 725/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0346 - accuracy: 0.9905 - val_loss: 1.1815 - val_accuracy: 0.8595\n",
            "Epoch 726/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 1.1843 - val_accuracy: 0.8595\n",
            "Epoch 727/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 1.2043 - val_accuracy: 0.8558\n",
            "Epoch 728/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 1.1832 - val_accuracy: 0.8622\n",
            "Epoch 729/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0359 - accuracy: 0.9905 - val_loss: 1.1966 - val_accuracy: 0.8579\n",
            "Epoch 730/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9949 - val_loss: 1.3503 - val_accuracy: 0.8542\n",
            "Epoch 731/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9913 - val_loss: 1.2728 - val_accuracy: 0.8627\n",
            "Epoch 732/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 1.1656 - val_accuracy: 0.8590\n",
            "Epoch 733/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0149 - accuracy: 0.9964 - val_loss: 1.1986 - val_accuracy: 0.8584\n",
            "Epoch 734/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9933 - val_loss: 1.2072 - val_accuracy: 0.8494\n",
            "Epoch 735/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 1.2431 - val_accuracy: 0.8574\n",
            "Epoch 736/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0356 - accuracy: 0.9884 - val_loss: 1.1594 - val_accuracy: 0.8686\n",
            "Epoch 737/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0254 - accuracy: 0.9914 - val_loss: 1.1996 - val_accuracy: 0.8611\n",
            "Epoch 738/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0321 - accuracy: 0.9906 - val_loss: 1.1671 - val_accuracy: 0.8616\n",
            "Epoch 739/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9941 - val_loss: 1.1421 - val_accuracy: 0.8622\n",
            "Epoch 740/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 1.1407 - val_accuracy: 0.8659\n",
            "Epoch 741/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 1.1435 - val_accuracy: 0.8632\n",
            "Epoch 742/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0680 - accuracy: 0.9806 - val_loss: 1.2238 - val_accuracy: 0.8584\n",
            "Epoch 743/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0288 - accuracy: 0.9914 - val_loss: 1.2558 - val_accuracy: 0.8531\n",
            "Epoch 744/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0229 - accuracy: 0.9931 - val_loss: 1.1901 - val_accuracy: 0.8632\n",
            "Epoch 745/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 1.1737 - val_accuracy: 0.8600\n",
            "Epoch 746/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0216 - accuracy: 0.9947 - val_loss: 1.2069 - val_accuracy: 0.8622\n",
            "Epoch 747/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0525 - accuracy: 0.9854 - val_loss: 1.2335 - val_accuracy: 0.8542\n",
            "Epoch 748/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 1.1450 - val_accuracy: 0.8606\n",
            "Epoch 749/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 1.1856 - val_accuracy: 0.8574\n",
            "Epoch 750/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 1.2893 - val_accuracy: 0.8456\n",
            "Epoch 751/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 1.2086 - val_accuracy: 0.8590\n",
            "Epoch 752/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0337 - accuracy: 0.9919 - val_loss: 1.2365 - val_accuracy: 0.8600\n",
            "Epoch 753/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0205 - accuracy: 0.9944 - val_loss: 1.1825 - val_accuracy: 0.8611\n",
            "Epoch 754/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 1.1705 - val_accuracy: 0.8643\n",
            "Epoch 755/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0536 - accuracy: 0.9862 - val_loss: 1.2567 - val_accuracy: 0.8600\n",
            "Epoch 756/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 1.1951 - val_accuracy: 0.8531\n",
            "Epoch 757/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0245 - accuracy: 0.9921 - val_loss: 1.2485 - val_accuracy: 0.8616\n",
            "Epoch 758/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0146 - accuracy: 0.9964 - val_loss: 1.2380 - val_accuracy: 0.8574\n",
            "Epoch 759/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0301 - accuracy: 0.9910 - val_loss: 1.2466 - val_accuracy: 0.8622\n",
            "Epoch 760/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0432 - accuracy: 0.9897 - val_loss: 1.3474 - val_accuracy: 0.8515\n",
            "Epoch 761/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 1.2114 - val_accuracy: 0.8558\n",
            "Epoch 762/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0126 - accuracy: 0.9968 - val_loss: 1.1664 - val_accuracy: 0.8632\n",
            "Epoch 763/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 1.2439 - val_accuracy: 0.8558\n",
            "Epoch 764/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 1.2749 - val_accuracy: 0.8558\n",
            "Epoch 765/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0357 - accuracy: 0.9872 - val_loss: 1.2494 - val_accuracy: 0.8563\n",
            "Epoch 766/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 1.1734 - val_accuracy: 0.8616\n",
            "Epoch 767/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 1.2792 - val_accuracy: 0.8542\n",
            "Epoch 768/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0569 - accuracy: 0.9825 - val_loss: 1.3340 - val_accuracy: 0.8563\n",
            "Epoch 769/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 1.1962 - val_accuracy: 0.8595\n",
            "Epoch 770/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0205 - accuracy: 0.9953 - val_loss: 1.1994 - val_accuracy: 0.8590\n",
            "Epoch 771/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0252 - accuracy: 0.9928 - val_loss: 1.1802 - val_accuracy: 0.8611\n",
            "Epoch 772/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0168 - accuracy: 0.9956 - val_loss: 1.2479 - val_accuracy: 0.8632\n",
            "Epoch 773/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0537 - accuracy: 0.9833 - val_loss: 1.1684 - val_accuracy: 0.8574\n",
            "Epoch 774/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0159 - accuracy: 0.9955 - val_loss: 1.1626 - val_accuracy: 0.8568\n",
            "Epoch 775/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0202 - accuracy: 0.9944 - val_loss: 1.2028 - val_accuracy: 0.8627\n",
            "Epoch 776/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0151 - accuracy: 0.9959 - val_loss: 1.1905 - val_accuracy: 0.8632\n",
            "Epoch 777/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0345 - accuracy: 0.9920 - val_loss: 1.3407 - val_accuracy: 0.8446\n",
            "Epoch 778/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0355 - accuracy: 0.9905 - val_loss: 1.2214 - val_accuracy: 0.8659\n",
            "Epoch 779/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 1.3159 - val_accuracy: 0.8446\n",
            "Epoch 780/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 1.2946 - val_accuracy: 0.8579\n",
            "Epoch 781/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0324 - accuracy: 0.9909 - val_loss: 1.3061 - val_accuracy: 0.8547\n",
            "Epoch 782/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0367 - accuracy: 0.9909 - val_loss: 1.2340 - val_accuracy: 0.8579\n",
            "Epoch 783/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0201 - accuracy: 0.9940 - val_loss: 1.2231 - val_accuracy: 0.8606\n",
            "Epoch 784/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0211 - accuracy: 0.9935 - val_loss: 1.1750 - val_accuracy: 0.8606\n",
            "Epoch 785/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0227 - accuracy: 0.9924 - val_loss: 1.2884 - val_accuracy: 0.8504\n",
            "Epoch 786/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0205 - accuracy: 0.9929 - val_loss: 1.2631 - val_accuracy: 0.8595\n",
            "Epoch 787/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 1.1865 - val_accuracy: 0.8632\n",
            "Epoch 788/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0500 - accuracy: 0.9880 - val_loss: 1.2448 - val_accuracy: 0.8488\n",
            "Epoch 789/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0353 - accuracy: 0.9906 - val_loss: 1.2285 - val_accuracy: 0.8590\n",
            "Epoch 790/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 1.1665 - val_accuracy: 0.8707\n",
            "Epoch 791/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 1.2849 - val_accuracy: 0.8568\n",
            "Epoch 792/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0430 - accuracy: 0.9881 - val_loss: 1.2605 - val_accuracy: 0.8579\n",
            "Epoch 793/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0298 - accuracy: 0.9916 - val_loss: 1.2221 - val_accuracy: 0.8552\n",
            "Epoch 794/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 1.2831 - val_accuracy: 0.8467\n",
            "Epoch 795/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 1.1881 - val_accuracy: 0.8632\n",
            "Epoch 796/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0318 - accuracy: 0.9909 - val_loss: 1.2724 - val_accuracy: 0.8611\n",
            "Epoch 797/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0464 - accuracy: 0.9857 - val_loss: 1.2257 - val_accuracy: 0.8574\n",
            "Epoch 798/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 1.1838 - val_accuracy: 0.8702\n",
            "Epoch 799/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 1.1658 - val_accuracy: 0.8697\n",
            "Epoch 800/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0295 - accuracy: 0.9921 - val_loss: 1.2876 - val_accuracy: 0.8531\n",
            "Epoch 801/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0447 - accuracy: 0.9869 - val_loss: 1.2498 - val_accuracy: 0.8590\n",
            "Epoch 802/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0286 - accuracy: 0.9910 - val_loss: 1.2259 - val_accuracy: 0.8590\n",
            "Epoch 803/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0112 - accuracy: 0.9964 - val_loss: 1.1629 - val_accuracy: 0.8638\n",
            "Epoch 804/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0155 - accuracy: 0.9960 - val_loss: 1.2211 - val_accuracy: 0.8627\n",
            "Epoch 805/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9904 - val_loss: 1.3279 - val_accuracy: 0.8515\n",
            "Epoch 806/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0409 - accuracy: 0.9892 - val_loss: 1.2031 - val_accuracy: 0.8659\n",
            "Epoch 807/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0156 - accuracy: 0.9959 - val_loss: 1.2916 - val_accuracy: 0.8606\n",
            "Epoch 808/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0201 - accuracy: 0.9944 - val_loss: 1.2390 - val_accuracy: 0.8600\n",
            "Epoch 809/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9933 - val_loss: 1.3510 - val_accuracy: 0.8595\n",
            "Epoch 810/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 1.1237 - val_accuracy: 0.8697\n",
            "Epoch 811/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 1.1725 - val_accuracy: 0.8606\n",
            "Epoch 812/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0400 - accuracy: 0.9880 - val_loss: 1.2516 - val_accuracy: 0.8510\n",
            "Epoch 813/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0371 - accuracy: 0.9893 - val_loss: 1.2483 - val_accuracy: 0.8627\n",
            "Epoch 814/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 1.2039 - val_accuracy: 0.8638\n",
            "Epoch 815/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0161 - accuracy: 0.9956 - val_loss: 1.3113 - val_accuracy: 0.8584\n",
            "Epoch 816/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0490 - accuracy: 0.9866 - val_loss: 1.2287 - val_accuracy: 0.8600\n",
            "Epoch 817/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 1.2373 - val_accuracy: 0.8627\n",
            "Epoch 818/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0292 - accuracy: 0.9923 - val_loss: 1.2580 - val_accuracy: 0.8627\n",
            "Epoch 819/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0272 - accuracy: 0.9933 - val_loss: 1.2482 - val_accuracy: 0.8574\n",
            "Epoch 820/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0307 - accuracy: 0.9905 - val_loss: 1.2573 - val_accuracy: 0.8552\n",
            "Epoch 821/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0186 - accuracy: 0.9949 - val_loss: 1.2452 - val_accuracy: 0.8483\n",
            "Epoch 822/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 1.1927 - val_accuracy: 0.8606\n",
            "Epoch 823/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 1.2616 - val_accuracy: 0.8643\n",
            "Epoch 824/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 1.2686 - val_accuracy: 0.8552\n",
            "Epoch 825/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9924 - val_loss: 1.2714 - val_accuracy: 0.8627\n",
            "Epoch 826/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0367 - accuracy: 0.9894 - val_loss: 1.3578 - val_accuracy: 0.8504\n",
            "Epoch 827/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 1.2544 - val_accuracy: 0.8611\n",
            "Epoch 828/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 1.2073 - val_accuracy: 0.8643\n",
            "Epoch 829/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 1.3067 - val_accuracy: 0.8520\n",
            "Epoch 830/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0424 - accuracy: 0.9881 - val_loss: 1.2924 - val_accuracy: 0.8531\n",
            "Epoch 831/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 1.2859 - val_accuracy: 0.8584\n",
            "Epoch 832/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0157 - accuracy: 0.9959 - val_loss: 1.2132 - val_accuracy: 0.8632\n",
            "Epoch 833/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0149 - accuracy: 0.9963 - val_loss: 1.3012 - val_accuracy: 0.8552\n",
            "Epoch 834/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0145 - accuracy: 0.9963 - val_loss: 1.4481 - val_accuracy: 0.8552\n",
            "Epoch 835/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0615 - accuracy: 0.9846 - val_loss: 1.3833 - val_accuracy: 0.8600\n",
            "Epoch 836/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0512 - accuracy: 0.9878 - val_loss: 1.2968 - val_accuracy: 0.8531\n",
            "Epoch 837/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 1.2583 - val_accuracy: 0.8632\n",
            "Epoch 838/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0106 - accuracy: 0.9968 - val_loss: 1.2513 - val_accuracy: 0.8600\n",
            "Epoch 839/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0041 - accuracy: 0.9984 - val_loss: 1.2233 - val_accuracy: 0.8584\n",
            "Epoch 840/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 1.2364 - val_accuracy: 0.8632\n",
            "Epoch 841/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0330 - accuracy: 0.9910 - val_loss: 1.3878 - val_accuracy: 0.8499\n",
            "Epoch 842/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0647 - accuracy: 0.9813 - val_loss: 1.2481 - val_accuracy: 0.8649\n",
            "Epoch 843/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 1.2732 - val_accuracy: 0.8675\n",
            "Epoch 844/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0266 - accuracy: 0.9929 - val_loss: 1.2333 - val_accuracy: 0.8600\n",
            "Epoch 845/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 1.2081 - val_accuracy: 0.8665\n",
            "Epoch 846/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 1.2072 - val_accuracy: 0.8632\n",
            "Epoch 847/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0540 - accuracy: 0.9873 - val_loss: 1.5349 - val_accuracy: 0.8467\n",
            "Epoch 848/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0420 - accuracy: 0.9882 - val_loss: 1.2484 - val_accuracy: 0.8611\n",
            "Epoch 849/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 1.2561 - val_accuracy: 0.8542\n",
            "Epoch 850/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0092 - accuracy: 0.9977 - val_loss: 1.2441 - val_accuracy: 0.8643\n",
            "Epoch 851/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 1.3252 - val_accuracy: 0.8574\n",
            "Epoch 852/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0444 - accuracy: 0.9873 - val_loss: 1.3767 - val_accuracy: 0.8558\n",
            "Epoch 853/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0408 - accuracy: 0.9888 - val_loss: 1.2576 - val_accuracy: 0.8649\n",
            "Epoch 854/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0277 - accuracy: 0.9925 - val_loss: 1.2594 - val_accuracy: 0.8584\n",
            "Epoch 855/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 1.2246 - val_accuracy: 0.8707\n",
            "Epoch 856/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 1.2324 - val_accuracy: 0.8611\n",
            "Epoch 857/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0322 - accuracy: 0.9944 - val_loss: 1.3806 - val_accuracy: 0.8478\n",
            "Epoch 858/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0412 - accuracy: 0.9893 - val_loss: 1.2624 - val_accuracy: 0.8670\n",
            "Epoch 859/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 1.2548 - val_accuracy: 0.8590\n",
            "Epoch 860/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9956 - val_loss: 1.3506 - val_accuracy: 0.8462\n",
            "Epoch 861/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 1.2236 - val_accuracy: 0.8568\n",
            "Epoch 862/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0329 - accuracy: 0.9913 - val_loss: 1.4427 - val_accuracy: 0.8531\n",
            "Epoch 863/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 1.3147 - val_accuracy: 0.8526\n",
            "Epoch 864/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9935 - val_loss: 1.3246 - val_accuracy: 0.8590\n",
            "Epoch 865/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 1.2608 - val_accuracy: 0.8563\n",
            "Epoch 866/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9949 - val_loss: 1.2802 - val_accuracy: 0.8584\n",
            "Epoch 867/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 1.2342 - val_accuracy: 0.8563\n",
            "Epoch 868/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0110 - accuracy: 0.9977 - val_loss: 1.2966 - val_accuracy: 0.8574\n",
            "Epoch 869/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0583 - accuracy: 0.9869 - val_loss: 1.4119 - val_accuracy: 0.8536\n",
            "Epoch 870/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 1.3133 - val_accuracy: 0.8606\n",
            "Epoch 871/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0120 - accuracy: 0.9972 - val_loss: 1.3202 - val_accuracy: 0.8547\n",
            "Epoch 872/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 1.3542 - val_accuracy: 0.8584\n",
            "Epoch 873/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0505 - accuracy: 0.9864 - val_loss: 1.4471 - val_accuracy: 0.8515\n",
            "Epoch 874/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 1.3303 - val_accuracy: 0.8638\n",
            "Epoch 875/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0107 - accuracy: 0.9961 - val_loss: 1.2834 - val_accuracy: 0.8665\n",
            "Epoch 876/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0119 - accuracy: 0.9973 - val_loss: 1.3400 - val_accuracy: 0.8654\n",
            "Epoch 877/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0383 - accuracy: 0.9892 - val_loss: 1.3318 - val_accuracy: 0.8600\n",
            "Epoch 878/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0165 - accuracy: 0.9941 - val_loss: 1.3339 - val_accuracy: 0.8526\n",
            "Epoch 879/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0316 - accuracy: 0.9908 - val_loss: 1.2775 - val_accuracy: 0.8638\n",
            "Epoch 880/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 1.2548 - val_accuracy: 0.8686\n",
            "Epoch 881/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 1.2883 - val_accuracy: 0.8574\n",
            "Epoch 882/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0666 - accuracy: 0.9836 - val_loss: 1.3833 - val_accuracy: 0.8574\n",
            "Epoch 883/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0262 - accuracy: 0.9919 - val_loss: 1.3166 - val_accuracy: 0.8622\n",
            "Epoch 884/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0109 - accuracy: 0.9967 - val_loss: 1.2364 - val_accuracy: 0.8702\n",
            "Epoch 885/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 1.2496 - val_accuracy: 0.8665\n",
            "Epoch 886/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0127 - accuracy: 0.9976 - val_loss: 1.4578 - val_accuracy: 0.8520\n",
            "Epoch 887/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0758 - accuracy: 0.9788 - val_loss: 1.3316 - val_accuracy: 0.8600\n",
            "Epoch 888/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 1.2657 - val_accuracy: 0.8622\n",
            "Epoch 889/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0207 - accuracy: 0.9951 - val_loss: 1.3053 - val_accuracy: 0.8606\n",
            "Epoch 890/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0133 - accuracy: 0.9964 - val_loss: 1.2639 - val_accuracy: 0.8600\n",
            "Epoch 891/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 1.2986 - val_accuracy: 0.8606\n",
            "Epoch 892/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0259 - accuracy: 0.9924 - val_loss: 1.3943 - val_accuracy: 0.8526\n",
            "Epoch 893/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0471 - accuracy: 0.9860 - val_loss: 1.3989 - val_accuracy: 0.8574\n",
            "Epoch 894/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0180 - accuracy: 0.9956 - val_loss: 1.3489 - val_accuracy: 0.8552\n",
            "Epoch 895/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0342 - accuracy: 0.9919 - val_loss: 1.2725 - val_accuracy: 0.8590\n",
            "Epoch 896/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 1.3076 - val_accuracy: 0.8606\n",
            "Epoch 897/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 1.2878 - val_accuracy: 0.8590\n",
            "Epoch 898/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0203 - accuracy: 0.9944 - val_loss: 1.3118 - val_accuracy: 0.8590\n",
            "Epoch 899/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0488 - accuracy: 0.9869 - val_loss: 1.3753 - val_accuracy: 0.8552\n",
            "Epoch 900/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0219 - accuracy: 0.9945 - val_loss: 1.2812 - val_accuracy: 0.8649\n",
            "Epoch 901/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0267 - accuracy: 0.9941 - val_loss: 1.3237 - val_accuracy: 0.8579\n",
            "Epoch 902/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 1.3626 - val_accuracy: 0.8531\n",
            "Epoch 903/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0269 - accuracy: 0.9919 - val_loss: 1.3904 - val_accuracy: 0.8552\n",
            "Epoch 904/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 1.4499 - val_accuracy: 0.8590\n",
            "Epoch 905/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0334 - accuracy: 0.9906 - val_loss: 1.4179 - val_accuracy: 0.8611\n",
            "Epoch 906/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 1.3830 - val_accuracy: 0.8595\n",
            "Epoch 907/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0129 - accuracy: 0.9964 - val_loss: 1.3077 - val_accuracy: 0.8622\n",
            "Epoch 908/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0231 - accuracy: 0.9955 - val_loss: 1.4145 - val_accuracy: 0.8552\n",
            "Epoch 909/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0421 - accuracy: 0.9881 - val_loss: 1.3058 - val_accuracy: 0.8531\n",
            "Epoch 910/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 1.3690 - val_accuracy: 0.8606\n",
            "Epoch 911/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 1.3167 - val_accuracy: 0.8622\n",
            "Epoch 912/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 1.3937 - val_accuracy: 0.8643\n",
            "Epoch 913/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0219 - accuracy: 0.9945 - val_loss: 1.4709 - val_accuracy: 0.8606\n",
            "Epoch 914/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0103 - accuracy: 0.9972 - val_loss: 1.3285 - val_accuracy: 0.8670\n",
            "Epoch 915/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 1.3043 - val_accuracy: 0.8627\n",
            "Epoch 916/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 1.3083 - val_accuracy: 0.8643\n",
            "Epoch 917/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 1.3267 - val_accuracy: 0.8702\n",
            "Epoch 918/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0686 - accuracy: 0.9844 - val_loss: 1.5459 - val_accuracy: 0.8723\n",
            "Epoch 919/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0590 - accuracy: 0.9856 - val_loss: 1.3303 - val_accuracy: 0.8654\n",
            "Epoch 920/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9968 - val_loss: 1.3569 - val_accuracy: 0.8643\n",
            "Epoch 921/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0071 - accuracy: 0.9984 - val_loss: 1.3381 - val_accuracy: 0.8600\n",
            "Epoch 922/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 1.3198 - val_accuracy: 0.8659\n",
            "Epoch 923/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0519 - accuracy: 0.9870 - val_loss: 1.3656 - val_accuracy: 0.8616\n",
            "Epoch 924/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0185 - accuracy: 0.9933 - val_loss: 1.2896 - val_accuracy: 0.8659\n",
            "Epoch 925/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0209 - accuracy: 0.9952 - val_loss: 1.3181 - val_accuracy: 0.8638\n",
            "Epoch 926/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 1.2959 - val_accuracy: 0.8702\n",
            "Epoch 927/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0350 - accuracy: 0.9919 - val_loss: 1.3417 - val_accuracy: 0.8600\n",
            "Epoch 928/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0291 - accuracy: 0.9888 - val_loss: 1.3602 - val_accuracy: 0.8622\n",
            "Epoch 929/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0145 - accuracy: 0.9965 - val_loss: 1.2867 - val_accuracy: 0.8627\n",
            "Epoch 930/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 1.3125 - val_accuracy: 0.8632\n",
            "Epoch 931/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0262 - accuracy: 0.9919 - val_loss: 1.3139 - val_accuracy: 0.8654\n",
            "Epoch 932/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0173 - accuracy: 0.9948 - val_loss: 1.3230 - val_accuracy: 0.8579\n",
            "Epoch 933/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 1.3361 - val_accuracy: 0.8616\n",
            "Epoch 934/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0265 - accuracy: 0.9925 - val_loss: 1.4134 - val_accuracy: 0.8515\n",
            "Epoch 935/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9941 - val_loss: 1.2982 - val_accuracy: 0.8600\n",
            "Epoch 936/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0203 - accuracy: 0.9964 - val_loss: 1.4037 - val_accuracy: 0.8595\n",
            "Epoch 937/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 1.4969 - val_accuracy: 0.8472\n",
            "Epoch 938/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0314 - accuracy: 0.9912 - val_loss: 1.4457 - val_accuracy: 0.8531\n",
            "Epoch 939/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 1.3399 - val_accuracy: 0.8536\n",
            "Epoch 940/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 1.2992 - val_accuracy: 0.8584\n",
            "Epoch 941/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 1.3189 - val_accuracy: 0.8611\n",
            "Epoch 942/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 1.2739 - val_accuracy: 0.8632\n",
            "Epoch 943/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 1.3584 - val_accuracy: 0.8515\n",
            "Epoch 944/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0598 - accuracy: 0.9841 - val_loss: 1.4805 - val_accuracy: 0.8558\n",
            "Epoch 945/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0226 - accuracy: 0.9931 - val_loss: 1.3555 - val_accuracy: 0.8665\n",
            "Epoch 946/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 1.3427 - val_accuracy: 0.8595\n",
            "Epoch 947/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 1.3660 - val_accuracy: 0.8649\n",
            "Epoch 948/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 1.3994 - val_accuracy: 0.8600\n",
            "Epoch 949/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0644 - accuracy: 0.9848 - val_loss: 1.4429 - val_accuracy: 0.8584\n",
            "Epoch 950/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0253 - accuracy: 0.9932 - val_loss: 1.3518 - val_accuracy: 0.8590\n",
            "Epoch 951/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0264 - accuracy: 0.9929 - val_loss: 1.4155 - val_accuracy: 0.8478\n",
            "Epoch 952/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 1.3545 - val_accuracy: 0.8611\n",
            "Epoch 953/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 1.3332 - val_accuracy: 0.8649\n",
            "Epoch 954/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 1.3280 - val_accuracy: 0.8574\n",
            "Epoch 955/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0135 - accuracy: 0.9968 - val_loss: 1.3509 - val_accuracy: 0.8563\n",
            "Epoch 956/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0559 - accuracy: 0.9853 - val_loss: 1.3970 - val_accuracy: 0.8494\n",
            "Epoch 957/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 1.3466 - val_accuracy: 0.8579\n",
            "Epoch 958/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 1.3100 - val_accuracy: 0.8670\n",
            "Epoch 959/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 1.4027 - val_accuracy: 0.8590\n",
            "Epoch 960/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 1.3217 - val_accuracy: 0.8638\n",
            "Epoch 961/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0441 - accuracy: 0.9876 - val_loss: 1.3597 - val_accuracy: 0.8611\n",
            "Epoch 962/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 1.3567 - val_accuracy: 0.8616\n",
            "Epoch 963/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 1.3084 - val_accuracy: 0.8654\n",
            "Epoch 964/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0246 - accuracy: 0.9935 - val_loss: 1.4940 - val_accuracy: 0.8488\n",
            "Epoch 965/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0347 - accuracy: 0.9881 - val_loss: 1.4369 - val_accuracy: 0.8515\n",
            "Epoch 966/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0196 - accuracy: 0.9944 - val_loss: 1.3845 - val_accuracy: 0.8595\n",
            "Epoch 967/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0186 - accuracy: 0.9944 - val_loss: 1.3732 - val_accuracy: 0.8611\n",
            "Epoch 968/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 1.5591 - val_accuracy: 0.8424\n",
            "Epoch 969/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0236 - accuracy: 0.9941 - val_loss: 1.4455 - val_accuracy: 0.8579\n",
            "Epoch 970/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0087 - accuracy: 0.9964 - val_loss: 1.3677 - val_accuracy: 0.8638\n",
            "Epoch 971/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0592 - accuracy: 0.9869 - val_loss: 1.4086 - val_accuracy: 0.8675\n",
            "Epoch 972/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0154 - accuracy: 0.9944 - val_loss: 1.3615 - val_accuracy: 0.8584\n",
            "Epoch 973/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 1.3349 - val_accuracy: 0.8584\n",
            "Epoch 974/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 1.3273 - val_accuracy: 0.8590\n",
            "Epoch 975/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 1.3788 - val_accuracy: 0.8542\n",
            "Epoch 976/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.9813 - val_loss: 1.5061 - val_accuracy: 0.8504\n",
            "Epoch 977/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9935 - val_loss: 1.3547 - val_accuracy: 0.8622\n",
            "Epoch 978/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 1.3512 - val_accuracy: 0.8632\n",
            "Epoch 979/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 1.3361 - val_accuracy: 0.8611\n",
            "Epoch 980/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9908 - val_loss: 1.4266 - val_accuracy: 0.8531\n",
            "Epoch 981/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 1.3826 - val_accuracy: 0.8558\n",
            "Epoch 982/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 1.3037 - val_accuracy: 0.8622\n",
            "Epoch 983/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 1.2617 - val_accuracy: 0.8670\n",
            "Epoch 984/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9963 - val_loss: 1.5873 - val_accuracy: 0.8413\n",
            "Epoch 985/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0623 - accuracy: 0.9858 - val_loss: 1.4928 - val_accuracy: 0.8494\n",
            "Epoch 986/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0283 - accuracy: 0.9919 - val_loss: 1.3631 - val_accuracy: 0.8627\n",
            "Epoch 987/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9959 - val_loss: 1.3650 - val_accuracy: 0.8510\n",
            "Epoch 988/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 1.3982 - val_accuracy: 0.8526\n",
            "Epoch 989/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9941 - val_loss: 1.4041 - val_accuracy: 0.8574\n",
            "Epoch 990/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0268 - accuracy: 0.9945 - val_loss: 1.3591 - val_accuracy: 0.8542\n",
            "Epoch 991/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9892 - val_loss: 1.3441 - val_accuracy: 0.8606\n",
            "Epoch 992/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 1.3506 - val_accuracy: 0.8526\n",
            "Epoch 993/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0086 - accuracy: 0.9981 - val_loss: 1.3689 - val_accuracy: 0.8558\n",
            "Epoch 994/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9905 - val_loss: 1.3625 - val_accuracy: 0.8638\n",
            "Epoch 995/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 1.3214 - val_accuracy: 0.8649\n",
            "Epoch 996/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 1.4120 - val_accuracy: 0.8568\n",
            "Epoch 997/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9885 - val_loss: 1.3939 - val_accuracy: 0.8606\n",
            "Epoch 998/1000\n",
            "749/749 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9952 - val_loss: 1.4365 - val_accuracy: 0.8504\n",
            "Epoch 999/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0200 - accuracy: 0.9933 - val_loss: 1.3801 - val_accuracy: 0.8547\n",
            "Epoch 1000/1000\n",
            "749/749 [==============================] - 1s 1ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 1.3686 - val_accuracy: 0.8632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCcV1Go3V6D9",
        "outputId": "6505cb6f-89be-4f12-a295-d533c88f1ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss_3, test_acc_3 = model3.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss %f, Accuracy %f' % (test_loss_1, test_acc_1))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_2, test_acc_2))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_3, test_acc_3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 815us/step - loss: 1.3686 - accuracy: 0.8632\n",
            "Loss 0.339812, Accuracy 0.849893\n",
            "Loss 0.374640, Accuracy 0.836004\n",
            "Loss 1.368555, Accuracy 0.863248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ-TAqsFV-jr",
        "outputId": "df6b5665-bc39-466e-bea6-c9c8bb654b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(history3['loss'], label='Train')\n",
        "plt.plot(history3['val_loss'], label='Val')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross-Entropy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itM0GavkWwbK"
      },
      "source": [
        "Come prevedibile riesco a ottenere un valore di Accuracy + alto = 0.863248 ma allo stesso tempo il valore di Loss è alzato drasticamente a 1.368555"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtTbkrQoXDQN"
      },
      "source": [
        "A questo punto la successiva analisi riguarda  l'evitare problemi di overfitting.\n",
        "Si è deciso quindi di applicare gli approcci di Early Stopping, L2 Regularization e Drop-Out e infine vedere tra questi 3 quale fosse il migliore\n",
        "in termini di accuratezza."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11nMMhcXYmW"
      },
      "source": [
        "### **Early Stopping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNakkvkSW8ib"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXYQzcbIXeC_"
      },
      "source": [
        "The most relevant arguments are:\n",
        "\n",
        "- monitor: quantity to be monitored\n",
        "- patience: number of epochs with no improvement after which training will be stopped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp44X8MkXrBB"
      },
      "source": [
        "per le operezioni successive il Dataset è stato splittato in training set e validation set, con rispettive percentuali del 80% e 20%; \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FttSaH9XXpVW"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzGHdhPLXidV",
        "outputId": "a9a6905f-2ddf-4d32-db2d-8c0e2dbe7f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', patience=10)\n",
        "mc = ModelCheckpoint('best_model_NOREG.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model4 = build_model()\n",
        "history4 = model4.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, \n",
        "                      batch_size=10, callbacks=[es,mc]).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.5356 - accuracy: 0.7261 - val_loss: 0.5070 - val_accuracy: 0.7615\n",
            "Epoch 2/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.5037 - accuracy: 0.7497 - val_loss: 0.5135 - val_accuracy: 0.7435\n",
            "Epoch 3/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.7637 - val_loss: 0.4853 - val_accuracy: 0.7842\n",
            "Epoch 4/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7624 - val_loss: 0.4858 - val_accuracy: 0.7802\n",
            "Epoch 5/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4667 - accuracy: 0.7685 - val_loss: 0.4841 - val_accuracy: 0.7862\n",
            "Epoch 6/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4583 - accuracy: 0.7754 - val_loss: 0.4657 - val_accuracy: 0.7902\n",
            "Epoch 7/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.7834 - val_loss: 0.4681 - val_accuracy: 0.7929\n",
            "Epoch 8/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4390 - accuracy: 0.7904 - val_loss: 0.4700 - val_accuracy: 0.7862\n",
            "Epoch 9/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4320 - accuracy: 0.7874 - val_loss: 0.4487 - val_accuracy: 0.7902\n",
            "Epoch 10/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4247 - accuracy: 0.7926 - val_loss: 0.4487 - val_accuracy: 0.8076\n",
            "Epoch 11/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4183 - accuracy: 0.7921 - val_loss: 0.4455 - val_accuracy: 0.7963\n",
            "Epoch 12/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.7989 - val_loss: 0.4412 - val_accuracy: 0.7996\n",
            "Epoch 13/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.4049 - accuracy: 0.8068 - val_loss: 0.4597 - val_accuracy: 0.7862\n",
            "Epoch 14/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3987 - accuracy: 0.8130 - val_loss: 0.4463 - val_accuracy: 0.8009\n",
            "Epoch 15/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3924 - accuracy: 0.8160 - val_loss: 0.4339 - val_accuracy: 0.8043\n",
            "Epoch 16/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3851 - accuracy: 0.8165 - val_loss: 0.4370 - val_accuracy: 0.8123\n",
            "Epoch 17/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3831 - accuracy: 0.8213 - val_loss: 0.4250 - val_accuracy: 0.8110\n",
            "Epoch 18/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3780 - accuracy: 0.8226 - val_loss: 0.4283 - val_accuracy: 0.8110\n",
            "Epoch 19/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3735 - accuracy: 0.8272 - val_loss: 0.4225 - val_accuracy: 0.8116\n",
            "Epoch 20/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3657 - accuracy: 0.8282 - val_loss: 0.4409 - val_accuracy: 0.8016\n",
            "Epoch 21/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3646 - accuracy: 0.8298 - val_loss: 0.4187 - val_accuracy: 0.8163\n",
            "Epoch 22/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3553 - accuracy: 0.8338 - val_loss: 0.4271 - val_accuracy: 0.8096\n",
            "Epoch 23/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3554 - accuracy: 0.8325 - val_loss: 0.4209 - val_accuracy: 0.8123\n",
            "Epoch 24/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3505 - accuracy: 0.8417 - val_loss: 0.4186 - val_accuracy: 0.8110\n",
            "Epoch 25/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3484 - accuracy: 0.8383 - val_loss: 0.4228 - val_accuracy: 0.8090\n",
            "Epoch 26/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3403 - accuracy: 0.8425 - val_loss: 0.4194 - val_accuracy: 0.8150\n",
            "Epoch 27/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3400 - accuracy: 0.8435 - val_loss: 0.4158 - val_accuracy: 0.8183\n",
            "Epoch 28/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8472 - val_loss: 0.4073 - val_accuracy: 0.8203\n",
            "Epoch 29/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3307 - accuracy: 0.8497 - val_loss: 0.4059 - val_accuracy: 0.8263\n",
            "Epoch 30/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3248 - accuracy: 0.8482 - val_loss: 0.4097 - val_accuracy: 0.8156\n",
            "Epoch 31/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3232 - accuracy: 0.8507 - val_loss: 0.4629 - val_accuracy: 0.8130\n",
            "Epoch 32/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3197 - accuracy: 0.8507 - val_loss: 0.4216 - val_accuracy: 0.8123\n",
            "Epoch 33/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3194 - accuracy: 0.8534 - val_loss: 0.4276 - val_accuracy: 0.8029\n",
            "Epoch 34/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3137 - accuracy: 0.8552 - val_loss: 0.3990 - val_accuracy: 0.8270\n",
            "Epoch 35/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3093 - accuracy: 0.8589 - val_loss: 0.4274 - val_accuracy: 0.8096\n",
            "Epoch 36/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3085 - accuracy: 0.8586 - val_loss: 0.4030 - val_accuracy: 0.8216\n",
            "Epoch 37/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3036 - accuracy: 0.8599 - val_loss: 0.4128 - val_accuracy: 0.8136\n",
            "Epoch 38/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.3052 - accuracy: 0.8575 - val_loss: 0.4215 - val_accuracy: 0.8283\n",
            "Epoch 39/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2956 - accuracy: 0.8647 - val_loss: 0.4170 - val_accuracy: 0.8183\n",
            "Epoch 40/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2941 - accuracy: 0.8649 - val_loss: 0.4130 - val_accuracy: 0.8170\n",
            "Epoch 41/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2939 - accuracy: 0.8662 - val_loss: 0.4348 - val_accuracy: 0.8090\n",
            "Epoch 42/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2919 - accuracy: 0.8682 - val_loss: 0.4210 - val_accuracy: 0.8103\n",
            "Epoch 43/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2876 - accuracy: 0.8681 - val_loss: 0.4271 - val_accuracy: 0.8203\n",
            "Epoch 44/150\n",
            "599/599 [==============================] - 1s 1ms/step - loss: 0.2831 - accuracy: 0.8727 - val_loss: 0.4288 - val_accuracy: 0.8143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXzh5Qacts_"
      },
      "source": [
        "si è stoppata a 44 su 150 epoche"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGkVnJAeYWpx",
        "outputId": "11683ecc-97df-4d8f-c51d-6cc8651b8c5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_loss_4, test_acc_4 = model4.evaluate(X_test, y_test)\n",
        "\n",
        "print('Loss %f, Accuracy %f' % (test_loss_1, test_acc_1))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_2, test_acc_2))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_3, test_acc_3))\n",
        "print('Loss %f, Accuracy %f' % (test_loss_4, test_acc_4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 830us/step - loss: 0.4038 - accuracy: 0.8248\n",
            "Loss 0.339812, Accuracy 0.849893\n",
            "Loss 0.374640, Accuracy 0.836004\n",
            "Loss 1.368555, Accuracy 0.863248\n",
            "Loss 0.403752, Accuracy 0.824786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvDKG7SoY98G"
      },
      "source": [
        "Il modello con Early Stopping ha dato un accuracy di 0.863248 e Loss 0.403752\n",
        "\n",
        "NOTA BENE: fatto meglio dopo tutti e 3 insieme la valutazione di accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PmSTZXsZFzr"
      },
      "source": [
        "### **L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVMnQj4sZEtc"
      },
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "def build_L2_model():\n",
        "    # define the model\n",
        "    model = Sequential()\n",
        "\n",
        "    n_feature = X_train.shape[1]\n",
        "    h_dim=100\n",
        "    model.add(Dense(h_dim, activation='relu', input_shape=(n_feature,), kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(h_dim, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(h_dim, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    model.add(Dense(h_dim, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "    #linear activation\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MdLpFWBcICf",
        "outputId": "02bb4633-0c89-47bf-d6b8-7968dec81379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mc = ModelCheckpoint('best_model_L2.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "L2_model = build_L2_model()\n",
        "h_L2 = L2_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, \n",
        "                    batch_size=10, callbacks=[es,mc]).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 1.7880 - accuracy: 0.7148 - val_loss: 1.0069 - val_accuracy: 0.7422\n",
            "Epoch 2/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.7892 - accuracy: 0.7276 - val_loss: 0.7058 - val_accuracy: 0.7408\n",
            "Epoch 3/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.6398 - accuracy: 0.7209 - val_loss: 0.5860 - val_accuracy: 0.7355\n",
            "Epoch 4/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5800 - accuracy: 0.7308 - val_loss: 0.5961 - val_accuracy: 0.7355\n",
            "Epoch 5/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5735 - accuracy: 0.7295 - val_loss: 0.6129 - val_accuracy: 0.7495\n",
            "Epoch 6/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5663 - accuracy: 0.7370 - val_loss: 0.6004 - val_accuracy: 0.7328\n",
            "Epoch 7/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5653 - accuracy: 0.7353 - val_loss: 0.5517 - val_accuracy: 0.7515\n",
            "Epoch 8/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5550 - accuracy: 0.7403 - val_loss: 0.5486 - val_accuracy: 0.7508\n",
            "Epoch 9/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5505 - accuracy: 0.7415 - val_loss: 0.5636 - val_accuracy: 0.7435\n",
            "Epoch 10/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5575 - accuracy: 0.7371 - val_loss: 0.5560 - val_accuracy: 0.7568\n",
            "Epoch 11/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.7408 - val_loss: 0.6139 - val_accuracy: 0.7555\n",
            "Epoch 12/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5960 - accuracy: 0.7159 - val_loss: 0.5660 - val_accuracy: 0.7408\n",
            "Epoch 13/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5698 - accuracy: 0.7260 - val_loss: 0.5574 - val_accuracy: 0.7408\n",
            "Epoch 14/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5555 - accuracy: 0.7400 - val_loss: 0.5726 - val_accuracy: 0.7535\n",
            "Epoch 15/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5515 - accuracy: 0.7435 - val_loss: 0.5945 - val_accuracy: 0.7281\n",
            "Epoch 16/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5646 - accuracy: 0.7368 - val_loss: 0.5595 - val_accuracy: 0.7422\n",
            "Epoch 17/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5500 - accuracy: 0.7406 - val_loss: 0.5531 - val_accuracy: 0.7542\n",
            "Epoch 18/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7485 - val_loss: 0.5466 - val_accuracy: 0.7582\n",
            "Epoch 19/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5456 - accuracy: 0.7433 - val_loss: 0.5618 - val_accuracy: 0.7602\n",
            "Epoch 20/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5513 - accuracy: 0.7503 - val_loss: 0.5700 - val_accuracy: 0.7568\n",
            "Epoch 21/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5557 - accuracy: 0.7473 - val_loss: 0.6134 - val_accuracy: 0.7555\n",
            "Epoch 22/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5532 - accuracy: 0.7386 - val_loss: 0.5478 - val_accuracy: 0.7595\n",
            "Epoch 23/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5496 - accuracy: 0.7415 - val_loss: 0.5747 - val_accuracy: 0.7562\n",
            "Epoch 24/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5502 - accuracy: 0.7533 - val_loss: 0.6187 - val_accuracy: 0.7582\n",
            "Epoch 25/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7510 - val_loss: 0.5758 - val_accuracy: 0.7555\n",
            "Epoch 26/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5495 - accuracy: 0.7452 - val_loss: 0.5639 - val_accuracy: 0.7615\n",
            "Epoch 27/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7473 - val_loss: 0.5628 - val_accuracy: 0.7615\n",
            "Epoch 28/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 0.7067 - accuracy: 0.7263 - val_loss: 0.6205 - val_accuracy: 0.7255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992UYLtXcoQ1"
      },
      "source": [
        "si è stoppata a 28 su 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfE3-yOcUg-"
      },
      "source": [
        "### **Drop-Out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0IJ55uTcV1A"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def build_DROPOUT_model():\n",
        "    # define the model\n",
        "    model = Sequential()\n",
        "\n",
        "    n_feature = X_train.shape[1]\n",
        "    h_dim=100\n",
        "    model.add(Dense(h_dim, activation='relu', input_shape=(n_feature,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(h_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(h_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(h_dim, activation='relu'))\n",
        "    #linear activation\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6TSkmnscnK1",
        "outputId": "a173e1bc-b5a7-4c5d-c188-4d18ad3a2b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mc = ModelCheckpoint('best_model_DROPOUT.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "DROPOUT_model = build_DROPOUT_model()\n",
        "h_DROPOUT = DROPOUT_model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
        "                              epochs=150, batch_size=10, callbacks=[es,mc]).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 2/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 3/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 4/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 5/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 6/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 7/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 8/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 9/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 10/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n",
            "Epoch 11/150\n",
            "599/599 [==============================] - 1s 2ms/step - loss: 4.5672 - accuracy: 0.7039 - val_loss: 4.2349 - val_accuracy: 0.7255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB-gr-1Bc1lw"
      },
      "source": [
        "Questo si è stoppato a 11 epoche su 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibm6GK3oc8Ge"
      },
      "source": [
        "### Conclusioni su i 3 approcci sopra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swM5Y_kyc1O7",
        "outputId": "3b6d7bb7-1d17-4fda-d31d-f07c8e1d1626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_NOREG_model = load_model('best_model_NOREG.h5')\n",
        "best_L2_model = load_model('best_model_L2.h5')\n",
        "best_DROPOUT_model = load_model('best_model_DROPOUT.h5')\n",
        "\n",
        "loss_NOREG, acc_NOREG = best_NOREG_model.evaluate(X_test, y_test)\n",
        "loss_L2, acc_L2 = best_L2_model.evaluate(X_test, y_test)\n",
        "loss_DROPOUT, acc_DROPOUT = best_DROPOUT_model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 796us/step - loss: 0.3865 - accuracy: 0.8216\n",
            "59/59 [==============================] - 0s 960us/step - loss: 0.5300 - accuracy: 0.7601\n",
            "59/59 [==============================] - 0s 948us/step - loss: 4.1529 - accuracy: 0.7308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcHV6xSBc7eM",
        "outputId": "b2f81d12-ede9-4a2a-bfdc-5725ec6c7652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Loss %f, Accuracy %f' % (loss_NOREG, acc_NOREG))\n",
        "print('Loss %f, Accuracy %f' % (loss_L2, acc_L2))\n",
        "print('Loss %f, Accuracy %f' % (loss_DROPOUT, acc_DROPOUT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss 0.386531, Accuracy 0.821581\n",
            "Loss 0.530034, Accuracy 0.760150\n",
            "Loss 4.152871, Accuracy 0.730769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa7lTH0LdKBI"
      },
      "source": [
        "**Tra i 3 approcci usati il migliore è Early Stopping con accuracy di 0.821581 e Loss 0.368531.**\n",
        "\n",
        "Gli altri due nettamente peggio, riportali in una tabellina per report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AusqP3XQdgcH"
      },
      "source": [
        "### **Hyper-parameter Tuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWtqyS8ydyja"
      },
      "source": [
        "valutazione iperparametri del modello Keras per l’ottimizzazione. Usata libreria Keras e per ricercare i migliori parametri si è stato utilizzato l’approccio RandomizedSearchCv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RIb1_uwdgJN"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-a0a41FdYD9"
      },
      "source": [
        "def build_model(n_layers=2, h_dim=64, activation='relu', optimizer='adam'):\n",
        "    # define the model\n",
        "    model = Sequential()\n",
        "\n",
        "    n_feature = X_train.shape[1]\n",
        "    \n",
        "    model.add(Dense(h_dim, activation=activation, input_shape=(n_feature,)))\n",
        "    for i in range(n_layers-1):\n",
        "        model.add(Dense(h_dim, activation=activation))\n",
        "    #linear activation\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpBxX6EmfRSw"
      },
      "source": [
        "Come optimizer ho imposto io 'adam' perchè con il codice del prof che passava ['adagrad', 'adam'] nel codice sotto non andava, non so perchè."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvkxWsKWh5pt"
      },
      "source": [
        "i parametri sono:\n",
        "- n_layers = 1, 2, 3\n",
        "- h_dim = 32, 64, 128\n",
        "- activation = 'relu', 'tanh'\n",
        "- optimizer = adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZVMfgjveCx3"
      },
      "source": [
        "n_layers = [1, 2, 3]\n",
        "h_dim = [32, 64, 128]\n",
        "activation = ['relu', 'tanh']\n",
        "#optimizer = ['adagrad', 'adam']\n",
        "params = dict(optimizer=optimizer, n_layers=n_layers, h_dim=h_dim, activation=activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbB7motKeM7t",
        "outputId": "39be0c42-f282-4ebd-c472-0eb3e670ac59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = KerasRegressor(build_fn=build_model)\n",
        "\n",
        "rnd = RandomizedSearchCV(estimator=model, param_distributions=params, n_iter=5, cv=3)\n",
        "rnd_result = rnd.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 817us/step - loss: 0.5032 - accuracy: 0.7540\n",
            "63/63 [==============================] - 0s 792us/step - loss: 0.5251 - accuracy: 0.7580\n",
            "63/63 [==============================] - 0s 823us/step - loss: 0.5275 - accuracy: 0.7680\n",
            "63/63 [==============================] - 0s 860us/step - loss: 0.6974 - accuracy: 0.7645\n",
            "63/63 [==============================] - 0s 822us/step - loss: 0.6747 - accuracy: 0.7650\n",
            "63/63 [==============================] - 0s 788us/step - loss: 0.6890 - accuracy: 0.7951\n",
            "63/63 [==============================] - 0s 755us/step - loss: 0.5183 - accuracy: 0.7395\n",
            "63/63 [==============================] - 0s 820us/step - loss: 0.4945 - accuracy: 0.7590\n",
            "63/63 [==============================] - 0s 791us/step - loss: 0.5201 - accuracy: 0.7615\n",
            "63/63 [==============================] - 0s 881us/step - loss: 1.0210 - accuracy: 0.6774\n",
            "63/63 [==============================] - 0s 871us/step - loss: 0.4648 - accuracy: 0.7796\n",
            "63/63 [==============================] - 0s 850us/step - loss: 0.5414 - accuracy: 0.7816\n",
            "63/63 [==============================] - 0s 787us/step - loss: 0.9530 - accuracy: 0.7340\n",
            "63/63 [==============================] - 0s 797us/step - loss: 0.7811 - accuracy: 0.7400\n",
            "63/63 [==============================] - 0s 752us/step - loss: 0.6285 - accuracy: 0.7670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_AvTOOKhtRP",
        "outputId": "58c2b67f-7e8f-4287-c19d-4f9ed32a236b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (-rnd_result.best_score_, rnd_result.best_params_))\n",
        "means = rnd_result.cv_results_['mean_test_score']\n",
        "stds = rnd_result.cv_results_['std_test_score']\n",
        "params = rnd_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (-mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.511004 using {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 64, 'activation': 'tanh'}\n",
            "0.518587 (0.010951) with: {'optimizer': 'adagrad', 'n_layers': 1, 'h_dim': 32, 'activation': 'tanh'}\n",
            "0.687030 (0.009361) with: {'optimizer': 'adagrad', 'n_layers': 3, 'h_dim': 32, 'activation': 'relu'}\n",
            "0.511004 (0.011664) with: {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 64, 'activation': 'tanh'}\n",
            "0.675757 (0.246150) with: {'optimizer': 'adam', 'n_layers': 3, 'h_dim': 32, 'activation': 'tanh'}\n",
            "0.787505 (0.132566) with: {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 128, 'activation': 'relu'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvlxrsILh2_n",
        "outputId": "9eefcfdf-234b-4662-abf2-67c33427689d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = rnd_result.best_estimator_.model\n",
        "\n",
        "loss, acc = clf.evaluate(X_test, y_test)\n",
        "print('Loss %f, Accuracy %f' % (loss, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 745us/step - loss: 0.4880 - accuracy: 0.7863\n",
            "Loss 0.488026, Accuracy 0.786325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_7xUCDNiEl9"
      },
      "source": [
        "Il miglior risultato ottenuto è :\n",
        "\n",
        "**Best: 0.511004 using {'optimizer': 'adam', 'n_layers': 1, 'h_dim': 64, 'activation': 'tanh'}**\n",
        "\n",
        "Con un Loss di 0.488026 e Accuracy di 0.786325"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhzdznR5iVTf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}